{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e88da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10025551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas_datareader.data as web\n",
    "import re\n",
    "\n",
    "def clean_event_data(event_file_path):\n",
    "    df_events = pd.read_csv(event_file_path)\n",
    "\n",
    "    df_events['Announced_Date_Orig'] = df_events['Announced']\n",
    "    df_events['Trade_Date_Orig'] = df_events['Trade Date']\n",
    "\n",
    "    df_events['Announced'] = pd.to_datetime(df_events['Announced'], errors='coerce')\n",
    "    df_events['Trade Date'] = pd.to_datetime(df_events['Trade Date'], errors='coerce')\n",
    "\n",
    "    anomaly_mask = (df_events['Announced'] == datetime(2023, 12, 2)) & (df_events['Trade Date'] == datetime(2022, 12, 16))\n",
    "\n",
    "    num_anomalies = anomaly_mask.sum()\n",
    "    if num_anomalies > 0:\n",
    "        print(f\"Correcting {num_anomalies} date anomalies (Announced: 2023-12-02, Trade Date: 2022-12-16) to Announced: 2022-12-02.\")\n",
    "        df_events.loc[anomaly_mask, 'Trade Date'] = datetime(2023, 12, 18)\n",
    "\n",
    "    def normalize_ticker(ticker_str):\n",
    "        if pd.isna(ticker_str):\n",
    "            return None\n",
    "        ticker_str = ticker_str.strip()\n",
    "\n",
    "        if ticker_str.endswith(\" US\"):\n",
    "            ticker_str = ticker_str[:-3]\n",
    "\n",
    "        if '.' in ticker_str:\n",
    "            parts = ticker_str.split('.')\n",
    "            if len(parts) == 2 and len(parts[1]) == 1:\n",
    "                if parts[1].isalpha() and parts[1].isupper():\n",
    "                    ticker_str = parts[0] + '-' + parts[1]\n",
    "\n",
    "        if bool(re.search(r\"[a-z]\", ticker_str)):\n",
    "            ticker_str = re.sub('[^A-Z]', '', ticker_str)\n",
    "        return ticker_str\n",
    "\n",
    "    df_events['Ticker_Orig'] = df_events['Ticker']\n",
    "    df_events['Normalized_Ticker'] = df_events['Ticker'].apply(normalize_ticker)\n",
    "\n",
    "    #remove delisted tickers\n",
    "    # GRALV just striaght up was never added to the S&P400 lol\n",
    "    # FBHS was replaced on the S&P500, announced dec 12\n",
    "    # PACW was merged into Banc of Cali, BANC\n",
    "    # SIX trades under FUN now after acquiring cedar entertainment, price didnt match so i just removed it\n",
    "    # SNDK also listed as SNDKV, not sure why\n",
    "    # NATL (NCR Atleos) joined the S&P 600 while its parent company NCR Voyix stayed in the S&P400\n",
    "    delisted_tickers = ['AGTI', 'CBTX', 'NEX', 'GRALV', 'HTA', 'FBHS', 'PACW', 'FTREV', 'SIX', 'AVID', 'SNDKV']\n",
    "    df_events = df_events[~df_events['Normalized_Ticker'].isin(delisted_tickers)]\n",
    "    #fix typo tickers:\n",
    "    typo_tickers = {'STHOV': 'STHO', 'CHK':'EXE', 'GEHCV':'GEHC'}\n",
    "    for bad_ticker, correct_ticker in typo_tickers.items():\n",
    "        df_events.loc[df_events['Normalized_Ticker'] == bad_ticker, 'Normalized_Ticker'] = correct_ticker\n",
    "\n",
    "    def clean_numeric_col(series, remove_chars):\n",
    "        series_str = series.astype(str)\n",
    "        for char in remove_chars:\n",
    "            series_str = series_str.str.replace(char, '', regex=False)\n",
    "        series_str = series_str.replace('-', np.nan, regex=False)\n",
    "        series_str = series_str.str.strip()\n",
    "        series_str.loc[series_str == ''] = np.nan # Replace empty strings\n",
    "        return pd.to_numeric(series_str, errors='coerce')\n",
    "\n",
    "    df_events['Last Px'] = clean_numeric_col(df_events['Last Px'], ['$'])\n",
    "    df_events['Shs to Trade'] = clean_numeric_col(df_events['Shs to Trade'], [','])\n",
    "    df_events['$MM to Trade'] = clean_numeric_col(df_events['$MM to Trade'], ['$', ','])\n",
    "    df_events['ADV to Trade Pct'] = df_events['ADV to Trade'].astype(str).str.replace('%', '', regex=False)\n",
    "    df_events['ADV to Trade Pct'] = clean_numeric_col(df_events['ADV to Trade Pct'], []) / 100.0\n",
    "\n",
    "    df_events.rename(columns={'Announced': 'Announce_Date', 'Trade Date': 'Trade_Date'}, inplace=True)\n",
    "\n",
    "    return df_events.sort_values(by='Announce_Date').reset_index(drop=True)\n",
    "\n",
    "# --- 2. Fetch FRED Data (Fed Funds Rate) ---\n",
    "def fetch_fred_data(start_date, end_date):\n",
    "    print(f\"Fetching FRED data (DFF) from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
    "    try:\n",
    "        ff_rate = web.DataReader('DFF', 'fred', start_date, end_date)\n",
    "        ff_rate.rename(columns={'DFF': 'Fed_Funds_Rate'}, inplace=True)\n",
    "        ff_rate['Fed_Funds_Rate'] = ff_rate['Fed_Funds_Rate'] / 100.0\n",
    "        # FRED data is daily. We need to ffill for weekends/holidays for merging.\n",
    "        # Create a complete date range to ensure all days are covered\n",
    "        all_days = pd.date_range(start=ff_rate.index.min(), end=ff_rate.index.max(), freq='D')\n",
    "        ff_rate = ff_rate.reindex(all_days).ffill()\n",
    "        return ff_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching FRED data: {e}\")\n",
    "        return pd.DataFrame(columns=['Fed_Funds_Rate']) # Return empty df on error\n",
    "\n",
    "\n",
    "# --- 3. Fetch Stock Price Data ---\n",
    "def fetch_price_data(tickers_list, start_date, end_date):\n",
    "    print(f\"Fetching stock price data for {len(tickers_list)} tickers from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
    "    # yfinance can be slow for many tickers one by one; batch download is better.\n",
    "    # It can also return a multi-index Panel, or a dict of DataFrames if group_by='ticker'.\n",
    "\n",
    "    # Some tickers might have issues (e.g., delisted, different symbol in yfinance)\n",
    "    # We'll download in batches to be more robust and identify problematic tickers\n",
    "    all_price_data = {}\n",
    "    problematic_tickers = []\n",
    "\n",
    "    # yfinance expects space-separated string for multiple tickers\n",
    "    # For very large lists, yfinance might still struggle.\n",
    "    # A more robust way is to loop, but it's slower. Let's try batch first.\n",
    "    try:\n",
    "        data = yf.download(tickers_list, start=start_date, end=end_date, progress=False, group_by='ticker')\n",
    "\n",
    "        for ticker in tickers_list:\n",
    "            if ticker in data and not data[ticker].empty:\n",
    "                all_price_data[ticker] = data[ticker][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "            elif ticker.replace('-', '.') in data and not data[ticker.replace('-', '.')].empty: # yfinance sometimes uses '.' e.g. BRK.B\n",
    "                all_price_data[ticker] = data[ticker.replace('-', '.')][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "            else:\n",
    "                # Try with common variations if primary failed\n",
    "                # e.g. if ticker was BRK.B and normalized to BRK-B, yfinance might still use BRK.B\n",
    "                alternative_ticker = ticker.replace('-', '.')\n",
    "                if alternative_ticker in data and not data[alternative_ticker].empty:\n",
    "                    all_price_data[ticker] = data[alternative_ticker][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "                else:\n",
    "                    # If still not found or empty after trying common variations, try individual download as a last resort\n",
    "                    try:\n",
    "                        single_data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "                        if not single_data.empty:\n",
    "                             all_price_data[ticker] = single_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "                        else:\n",
    "                            problematic_tickers.append(ticker)\n",
    "                    except Exception:\n",
    "                         problematic_tickers.append(ticker)\n",
    "\n",
    "        if not all_price_data:\n",
    "            print(\"No price data fetched. Check ticker symbols or date range.\")\n",
    "            return pd.DataFrame(), problematic_tickers\n",
    "\n",
    "        # Combine into a single multi-index DataFrame\n",
    "        combined_df = pd.concat(all_price_data, names=['Ticker', 'Date'])\n",
    "        print(combined_df)\n",
    "        combined_df.reset_index(inplace=True)\n",
    "        combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "        combined_df.set_index(['Date', 'Ticker'], inplace=True)\n",
    "        combined_df.sort_index(inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Major error during batch yfinance download: {e}. Some data may be missing.\")\n",
    "        # Fallback or iterative download might be needed if this is common\n",
    "        # For now, we'll proceed with what we got, if any\n",
    "        if not all_price_data:\n",
    "             return pd.DataFrame(), tickers_list # Assume all failed if major error and no data\n",
    "        combined_df = pd.concat(all_price_data, names=['Ticker', 'Date'])\n",
    "        combined_df.reset_index(inplace=True)\n",
    "        combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "        combined_df.set_index(['Date', 'Ticker'], inplace=True)\n",
    "        combined_df.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "    if problematic_tickers:\n",
    "        print(f\"Could not fetch data for the following {len(problematic_tickers)} tickers: {problematic_tickers}\")\n",
    "\n",
    "    return combined_df, problematic_tickers\n",
    "\n",
    "# --- 4. Calculate ADV ---\n",
    "def calculate_adv(price_df, window=20):\n",
    "    print(f\"Calculating {window}-day ADV...\")\n",
    "    if 'Volume' not in price_df.columns:\n",
    "        print(\"Volume data not found. Cannot calculate ADV.\")\n",
    "        price_df['ADV_20'] = np.nan\n",
    "        return price_df\n",
    "    # Calculate ADV: mean of past 'window' days' volume. Shift(1) to use data prior to current day.\n",
    "    price_df['ADV_20'] = price_df.groupby(level='Ticker')['Volume'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=max(1,window//2)).mean().shift(1) # min_periods to get some value even if not full window\n",
    "    )\n",
    "    return price_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d482db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Flow ---\n",
    "EVENT_FILE_PATH = \"idexAddData.csv\"\n",
    "\n",
    "df_events_cleaned = clean_event_data(EVENT_FILE_PATH)\n",
    "\n",
    "if not df_events_cleaned.empty:\n",
    "    print(f\"\\nCleaned Event Data Summary:\")\n",
    "    print(f\"Shape: {df_events_cleaned.shape}\")\n",
    "    print(f\"Date range (Announce_Date): {df_events_cleaned['Announce_Date'].min()} to {df_events_cleaned['Announce_Date'].max()}\")\n",
    "    print(f\"Date range (Trade_Date): {df_events_cleaned['Trade_Date'].min()} to {df_events_cleaned['Trade_Date'].max()}\")\n",
    "    print(f\"Unique normalized tickers: {df_events_cleaned['Normalized_Ticker'].nunique()}\")\n",
    "    # print(\"Sample of cleaned event data:\")\n",
    "    #print(df_events_cleaned[['Announce_Date', 'Trade_Date', 'Index', 'Ticker_Orig', 'Normalized_Ticker', 'Last Px', 'ADV to Trade Pct']].head())\n",
    "\n",
    "    # 2. Determine overall date range for fetching auxiliary data\n",
    "    # Buffer for ADV calculation (e.g., 20 trading days + buffer ~ 45 calendar days)\n",
    "    # Buffer for holding period (e.g., 1 month ~ 30 calendar days, plus another 30 for safety for max holding period)\n",
    "    data_fetch_start_date = df_events_cleaned['Announce_Date'].min() - timedelta(days=45)\n",
    "    data_fetch_end_date = df_events_cleaned['Trade_Date'].max() + timedelta(days=60) # Increased buffer for 1-month hold + slippage\n",
    "\n",
    "    # 3. Fetch FRED data\n",
    "    fred_rates = fetch_fred_data(data_fetch_start_date, data_fetch_end_date)\n",
    "    print(\"\\nSample of FRED data (Fed Funds Rate):\")\n",
    "    print(fred_rates.tail())\n",
    "\n",
    "    # 4. Fetch stock price data\n",
    "    unique_tickers_list = df_events_cleaned['Normalized_Ticker'].dropna().unique().tolist()\n",
    "\n",
    "    stock_price_data_multi, problematic_tickers = fetch_price_data(unique_tickers_list, data_fetch_start_date, data_fetch_end_date)\n",
    "\n",
    "    if not stock_price_data_multi.empty:\n",
    "        # print(\"\\nSample of fetched stock price data (MultiIndex):\")\n",
    "        # print(stock_price_data_multi.head())\n",
    "        # print(stock_price_data_multi.tail())\n",
    "\n",
    "        # 5. Calculate ADV\n",
    "        stock_price_data_multi = calculate_adv(stock_price_data_multi, window=20)\n",
    "        # print(\"\\nSample of stock price data with ADV_20:\")\n",
    "        # print(stock_price_data_multi.dropna(subset=['ADV_20']).head())\n",
    "\n",
    "        # --- Filter out events for which we couldn't get price data ---\n",
    "        original_event_count = len(df_events_cleaned)\n",
    "        # Get tickers for which we successfully fetched data\n",
    "        successful_tickers = stock_price_data_multi.index.get_level_values('Ticker').unique().tolist()\n",
    "        df_events_cleaned = df_events_cleaned[df_events_cleaned['Normalized_Ticker'].isin(successful_tickers)]\n",
    "        print(f\"\\nFiltered events to keep only those with available price data. Kept {len(df_events_cleaned)} events out of {original_event_count}.\")\n",
    "        print(f\"Dropped {original_event_count - len(df_events_cleaned)} events due to missing price data for their tickers.\")\n",
    "\n",
    "        # --- Ready for merging and strategy implementation ---\n",
    "        print(\"\\nData preparation complete. Ready for merging and strategy backtesting.\")\n",
    "        # For merging, we'll typically iterate through df_events_cleaned and pick relevant price data.\n",
    "        # Example: For each event, find price on Announce_Date + 1 day (Open), price on Effective_Date (Open/Close).\n",
    "        # Also merge Fed Funds rate for relevant dates.\n",
    "    else:\n",
    "        print(\"\\nFailed to fetch any stock price data. Cannot proceed with ADV calculation or backtesting.\")\n",
    "else:\n",
    "    print(\"\\nEvent data is empty after cleaning/filtering. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d300899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_data_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_cleaned['Entry_Date_Strat1'] = pd.NaT\n",
    "df_events_cleaned['Entry_Price_Strat1'] = np.nan\n",
    "df_events_cleaned['Exit_Date_Strat1'] = pd.NaT\n",
    "df_events_cleaned['Exit_Price_Strat1'] = np.nan\n",
    "df_events_cleaned['Pct_Change_Strat1'] = np.nan\n",
    "\n",
    "\n",
    "for index, event_row in df_events_cleaned.iterrows():\n",
    "    ticker = event_row['Normalized_Ticker']\n",
    "    announce_date = event_row['Announce_Date']\n",
    "    effective_date = event_row['Trade_Date']\n",
    "\n",
    "    # --- Determine Entry Date and Price ---\n",
    "    # Entry is the open on the first trading day *after* announce_date\n",
    "    # We query stock_price_data_multi for the ticker, for dates > announce_date, take the first one.\n",
    "\n",
    "    try:\n",
    "        # Select relevant ticker data and dates strictly after announcement\n",
    "        # Ensure the ticker exists in the stock_price_data_multi index\n",
    "        if ticker not in stock_price_data_multi.index.get_level_values('Ticker').unique():\n",
    "            # print(f\"Ticker {ticker} not found in stock_price_data_multi. Skipping event index {index}.\")\n",
    "            continue\n",
    "\n",
    "        potential_entry_days = stock_price_data_multi.loc[(slice(announce_date + pd.Timedelta(days=1), None), ticker), :]\n",
    "\n",
    "        if not potential_entry_days.empty:\n",
    "            actual_entry_day_data = potential_entry_days.iloc[0] # First trading day after announcement\n",
    "            entry_date = actual_entry_day_data.name[0] # Get the Date from the (Date, Ticker) index\n",
    "            entry_price = actual_entry_day_data['Open']\n",
    "\n",
    "            df_events_cleaned.loc[index, 'Entry_Date_Strat1'] = entry_date\n",
    "            df_events_cleaned.loc[index, 'Entry_Price_Strat1'] = entry_price\n",
    "        else:\n",
    "            print(f\"No price data found after Announce_Date for {ticker} (Event index {index}). Cannot determine entry price.\")\n",
    "            continue # Skip if no entry price\n",
    "\n",
    "        # --- Determine Exit Date and Price ---\n",
    "        # Exit is the close on the Effective_Date (which is 'Trade Date' in the project doc)\n",
    "        # The strategy note says \"Exit any day before or on the index 'trade date'\".\n",
    "        # For this analysis, we'll use \"close on trade date\" as the exit.\n",
    "        # For backtesting, we'll need to be more precise about exit timing.\n",
    "\n",
    "        df_events_cleaned.loc[index, 'Exit_Date_Strat1'] = effective_date # Storing the target exit date\n",
    "\n",
    "        # Check if effective_date exists for the ticker\n",
    "        if (effective_date, ticker) in stock_price_data_multi.index:\n",
    "            exit_price = stock_price_data_multi.loc[(effective_date, ticker), 'Close']\n",
    "            df_events_cleaned.loc[index, 'Exit_Price_Strat1'] = exit_price\n",
    "        else:\n",
    "            # If exact effective_date is not a trading day or no data, try to find the closest previous trading day's close.\n",
    "            # For this initial analysis of \"price on Effective_Date\", let's stick to exact.\n",
    "            # If data is missing, it will remain NaN.\n",
    "            print(f\"No price data found on Effective_Date {effective_date} for {ticker} (Event index {index}). Cannot determine exit price.\")\n",
    "            continue # Skip if no exit price for calculation\n",
    "\n",
    "        # --- Calculate Percentage Change ---\n",
    "        if pd.notna(entry_price) and pd.notna(exit_price) and entry_price != 0:\n",
    "            pct_change = (exit_price - entry_price) / entry_price\n",
    "            df_events_cleaned.loc[index, 'Pct_Change_Strat1'] = pct_change\n",
    "        else:\n",
    "            # print(f\"Could not calculate Pct_Change for {ticker} (Event index {index}) due to missing prices or zero entry price.\")\n",
    "            pass\n",
    "\n",
    "    except KeyError:\n",
    "        # This can happen if a ticker is in df_events_cleaned but not in stock_price_data_multi after all filters\n",
    "        # Or if indexing on dates fails unexpectedly.\n",
    "        # print(f\"KeyError encountered for ticker {ticker} or dates around event index {index}. Skipping.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred for ticker {ticker}, event index {index}: {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# Add the 'abs_price_change' column\n",
    "df_events_cleaned['Abs_Pct_Change_Strat1'] = df_events_cleaned['Pct_Change_Strat1'].abs()\n",
    "\n",
    "print(\"\\nCalculation complete. 'Pct_Change_Strat1' and 'Abs_Pct_Change_Strat1' added to df_events_cleaned.\")\n",
    "print(\"Summary of new columns:\")\n",
    "print(df_events_cleaned[['Announce_Date', 'Normalized_Ticker', 'Entry_Date_Strat1', 'Entry_Price_Strat1',\n",
    "                         'Trade_Date', 'Exit_Price_Strat1', 'Pct_Change_Strat1', 'Abs_Pct_Change_Strat1']].head())\n",
    "\n",
    "print(\"\\nDescriptive statistics for Pct_Change_Strat1:\")\n",
    "print(df_events_cleaned['Pct_Change_Strat1'].describe())\n",
    "\n",
    "print(\"\\nDescriptive statistics for Abs_Pct_Change_Strat1:\")\n",
    "print(df_events_cleaned['Abs_Pct_Change_Strat1'].describe())\n",
    "\n",
    "# Count of events where calculation was successful\n",
    "print(\"\\n doggy row\")\n",
    "df_events_cleaned.loc[df_events_cleaned['Pct_Change_Strat1'].isna(), 'Index'] = 'S&P 600'\n",
    "print(df_events_cleaned.loc[df_events_cleaned['Pct_Change_Strat1'].isna()])\n",
    "successful_calcs = df_events_cleaned['Pct_Change_Strat1'].notna().sum()\n",
    "total_events = len(df_events_cleaned)\n",
    "print(f\"\\nSuccessfully calculated price changes for {successful_calcs} out of {total_events} events processed for this step.\")\n",
    "#NATL(NCR Atleos spun off on OCT 17, first tradable date was on the 16th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05704079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Ensure 'Pct_Change_Strat1' is numeric and drop NaNs for plotting ---\n",
    "df_plot = df_events_cleaned[['Pct_Change_Strat1']].copy()\n",
    "df_plot.dropna(subset=['Pct_Change_Strat1'], inplace=True)\n",
    "df_plot['Pct_Change_Strat1'] = pd.to_numeric(df_plot['Pct_Change_Strat1'], errors='coerce')\n",
    "df_plot.dropna(subset=['Pct_Change_Strat1'], inplace=True)\n",
    "\n",
    "\n",
    "if df_plot.empty:\n",
    "    print(\"No data available for plotting after cleaning Pct_Change_Strat1.\")\n",
    "else:\n",
    "    # Separate positive and negative changes\n",
    "    positive_changes = df_plot[df_plot['Pct_Change_Strat1'] > 0]['Pct_Change_Strat1']\n",
    "    negative_changes = df_plot[df_plot['Pct_Change_Strat1'] < 0]['Pct_Change_Strat1']\n",
    "\n",
    "    print(f\"Number of positive changes: {len(positive_changes)}\")\n",
    "    print(f\"Number of negative changes: {len(negative_changes)}\")\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Determine a common bin range for better comparison, or use automatic binning.\n",
    "    # For percentage changes, a range like -0.2 to 0.2 might be reasonable, but let data guide.\n",
    "    # Let's find the overall min/max to set somewhat symmetrical bins, or let Matplotlib decide.\n",
    "\n",
    "    # Overall min and max to help with consistent binning if desired\n",
    "    # overall_min = df_plot['Pct_Change_Strat1'].min()\n",
    "    # overall_max = df_plot['Pct_Change_Strat1'].max()\n",
    "    # Forcing symmetrical x-axis can be good for comparison too.\n",
    "    # abs_max_val = max(abs(overall_min), abs(overall_max), 0.01) # Ensure not zero for limits\n",
    "\n",
    "    # Common number of bins\n",
    "    num_bins = 30\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-darkgrid') # Using a seaborn style for better aesthetics\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True) # Share Y-axis for easier comparison of counts\n",
    "\n",
    "    # Histogram for Positive Price Changes\n",
    "    if not positive_changes.empty:\n",
    "        axes[0].hist(positive_changes, bins=num_bins, color='green', edgecolor='black', alpha=0.7)\n",
    "        axes[0].set_title(f'Distribution of Positive Price Changes\\n(N={len(positive_changes)}, Mean={positive_changes.mean():.4f})')\n",
    "        axes[0].set_xlabel('Percentage Change (Post-Announcement to Effective Date)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        # Potentially set x-limits for positive side\n",
    "        # axes[0].set_xlim(0, max(positive_changes.max(), 0.1)) # Ensure x-axis starts at 0\n",
    "    else:\n",
    "        axes[0].set_title('No Positive Price Changes to Plot')\n",
    "        axes[0].text(0.5, 0.5, 'No Data', horizontalalignment='center', verticalalignment='center', transform=axes[0].transAxes)\n",
    "\n",
    "\n",
    "    # Histogram for Negative Price Changes\n",
    "    if not negative_changes.empty:\n",
    "        axes[1].hist(negative_changes, bins=num_bins, color='red', edgecolor='black', alpha=0.7)\n",
    "        axes[1].set_title(f'Distribution of Negative Price Changes\\n(N={len(negative_changes)}, Mean={negative_changes.mean():.4f})')\n",
    "        axes[1].set_xlabel('Percentage Change (Post-Announcement to Effective Date)')\n",
    "        # axes[1].set_ylabel('Frequency') # Y-label is shared\n",
    "        # Potentially set x-limits for negative side\n",
    "        # axes[1].set_xlim(min(negative_changes.min(), -0.1), 0) # Ensure x-axis ends at 0\n",
    "    else:\n",
    "        axes[1].set_title('No Negative Price Changes to Plot')\n",
    "        axes[1].text(0.5, 0.5, 'No Data', horizontalalignment='center', verticalalignment='center', transform=axes[1].transAxes)\n",
    "\n",
    "    # Set common x-axis limits based on the overall range of changes for better comparison\n",
    "    # This makes it easier to compare the spread of positive vs negative.\n",
    "    # For example, if positive max is 0.15 and negative min is -0.10, set xlim for positive to (0, 0.15) and negative to (-0.15, 0) or similar.\n",
    "    # Or let them auto-scale and observe.\n",
    "\n",
    "    # Let's try to make the scales comparable if desired.\n",
    "    # E.g., find max absolute deviation for setting limits.\n",
    "    if not positive_changes.empty and not negative_changes.empty:\n",
    "        max_abs_dev_pos = positive_changes.max()\n",
    "        max_abs_dev_neg = abs(negative_changes.min())\n",
    "        overall_max_abs_dev = max(max_abs_dev_pos, max_abs_dev_neg, 0.01) # Avoid zero if all changes are tiny\n",
    "\n",
    "        axes[0].set_xlim(0, overall_max_abs_dev * 1.05) # Add 5% buffer\n",
    "        axes[1].set_xlim(-overall_max_abs_dev * 1.05, 0) # Add 5% buffer\n",
    "    elif not positive_changes.empty:\n",
    "         axes[0].set_xlim(0, positive_changes.max() * 1.05 if positive_changes.max() > 0 else 0.01)\n",
    "    elif not negative_changes.empty:\n",
    "        axes[1].set_xlim(negative_changes.min() * 1.05 if negative_changes.min() < 0 else -0.01, 0)\n",
    "\n",
    "\n",
    "    fig.suptitle('Distribution of Price Changes: Announcement to Effective Date', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Statistics for Positive Changes ---\")\n",
    "    if not positive_changes.empty:\n",
    "        print(positive_changes.describe())\n",
    "    else:\n",
    "        print(\"No positive changes found.\")\n",
    "\n",
    "    print(\"\\n--- Statistics for Negative Changes ---\")\n",
    "    if not negative_changes.empty:\n",
    "        print(negative_changes.describe())\n",
    "    else:\n",
    "        print(\"No negative changes found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = df_events_cleaned['Pct_Change_Strat1'].dropna()\n",
    "\n",
    "if plot_data.empty:\n",
    "    print(\"No valid data available in 'Pct_Change_Strat1' to plot.\")\n",
    "else:\n",
    "    print(f\"Plotting histogram for 'Pct_Change_Strat1' based on {len(plot_data)} data points.\")\n",
    "\n",
    "    # Determine a reasonable number of bins. Freedman-Diaconis rule is often good.\n",
    "    # Or simply start with a common number like 30-50 and adjust.\n",
    "    # For simplicity here, let's use a fixed number, e.g., 50, or plt.hist's auto.\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.set_style(\"whitegrid\") # Seaborn style for better aesthetics\n",
    "\n",
    "    # Histogram using seaborn (which uses matplotlib backend)\n",
    "    # Using 'kde=True' will add a Kernel Density Estimate plot\n",
    "    sns.histplot(plot_data, bins='auto', kde=True, color='skyblue', edgecolor='black')\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title('Distribution of Percentage Price Changes (Post-Announcement to Effective Date)', fontsize=16)\n",
    "    plt.xlabel('Percentage Price Change (Pct_Change_Strat1)', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    mean_val = plot_data.mean()\n",
    "    median_val = plot_data.median()\n",
    "    plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_val:.4f}')\n",
    "    plt.axvline(median_val, color='green', linestyle='dashed', linewidth=2, label=f'Median: {median_val:.4f}')\n",
    "\n",
    "    # Add text annotations for mean and std dev\n",
    "    std_dev = plot_data.std()\n",
    "    skewness = plot_data.skew()\n",
    "    kurt = plot_data.kurtosis() # Fisher's kurtosis (normal is 0)\n",
    "\n",
    "    plt.text(0.05, 0.95, f'Mean: {mean_val:.4f}\\nStd Dev: {std_dev:.4f}\\nMedian: {median_val:.4f}',\n",
    "             transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
    "\n",
    "    plt.text(0.05, 0.80, f'Skewness: {skewness:.2f}\\nKurtosis: {kurt:.2f}\\nN: {len(plot_data)}',\n",
    "             transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='lightcoral', alpha=0.5))\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    plt.show()\n",
    "\n",
    "    # Additional descriptive statistics that are useful alongside the histogram\n",
    "    print(\"\\nFull Descriptive Statistics for Pct_Change_Strat1:\")\n",
    "    print(plot_data.describe(percentiles=[.01, .05, .1, .25, .5, .75, .9, .95, .99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"Calculating Initial Day Return and plotting scatter graph...\")\n",
    "\n",
    "# Initialize the new column\n",
    "df_events_cleaned['Initial_Day_Return'] = np.nan\n",
    "df_events_cleaned['Close_Price_Entry_Date_Strat1'] = np.nan\n",
    "\n",
    "\n",
    "for index, event_row in df_events_cleaned.iterrows():\n",
    "    ticker = event_row['Normalized_Ticker']\n",
    "    entry_date = event_row['Entry_Date_Strat1']\n",
    "    entry_price_open = event_row['Entry_Price_Strat1'] # This is Open on Entry_Date_Strat1\n",
    "\n",
    "    if pd.isna(entry_date) or pd.isna(entry_price_open) or entry_price_open == 0:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Fetch the close price on Entry_Date_Strat1\n",
    "        if (entry_date, ticker) in stock_price_data_multi.index:\n",
    "            close_price_entry_date = stock_price_data_multi.loc[(entry_date, ticker), 'Close']\n",
    "            df_events_cleaned.loc[index, 'Close_Price_Entry_Date_Strat1'] = close_price_entry_date\n",
    "\n",
    "            if pd.notna(close_price_entry_date):\n",
    "                initial_day_return = (close_price_entry_date - entry_price_open) / entry_price_open\n",
    "                df_events_cleaned.loc[index, 'Initial_Day_Return'] = initial_day_return\n",
    "        else:\n",
    "            # print(f\"Close price not found for {ticker} on {entry_date}. Cannot calculate Initial_Day_Return for event index {index}.\")\n",
    "            pass\n",
    "\n",
    "    except KeyError:\n",
    "        # print(f\"KeyError for {ticker} on {entry_date} while fetching close price. Event index {index}.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        # print(f\"Error calculating Initial_Day_Return for {ticker}, event index {index}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- Scatter Plot ---\n",
    "# Drop NaNs for plotting and correlation calculation\n",
    "plot_df = df_events_cleaned[['Initial_Day_Return', 'Pct_Change_Strat1']].dropna()\n",
    "\n",
    "if not plot_df.empty:\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(x='Initial_Day_Return', y='Pct_Change_Strat1', data=plot_df, alpha=0.6)\n",
    "\n",
    "    # Add a regression line to visualize the trend\n",
    "    sns.regplot(x='Initial_Day_Return', y='Pct_Change_Strat1', data=plot_df, scatter=False, color='red', line_kws={'label':\"Linear Reg.\"})\n",
    "\n",
    "    plt.title('Overall Drift vs. Initial Day Return Post-Announcement', fontsize=16)\n",
    "    plt.xlabel('Initial Day Return (on Entry_Date_Strat1)', fontsize=12)\n",
    "    plt.ylabel('Overall Pct Change (Entry to Effective Date)', fontsize=12)\n",
    "    plt.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    plt.axvline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    correlation, p_value = pearsonr(plot_df['Initial_Day_Return'], plot_df['Pct_Change_Strat1'])\n",
    "    print(f\"\\nPearson Correlation between Initial_Day_Return and Pct_Change_Strat1: {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"The correlation is statistically significant at the 0.05 level.\")\n",
    "    else:\n",
    "        print(\"The correlation is not statistically significant at the 0.05 level.\")\n",
    "\n",
    "    print(\"\\nDescriptive statistics for Initial_Day_Return:\")\n",
    "    print(df_events_cleaned['Initial_Day_Return'].describe())\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data to create scatter plot or calculate correlation after dropping NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c38695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"Calculating Initial N-Day Returns (2-day and 5-day)...\")\n",
    "\n",
    "# Initialize new columns\n",
    "df_events_cleaned['Initial_2Day_Return'] = np.nan\n",
    "df_events_cleaned['Initial_5Day_Return'] = np.nan\n",
    "\n",
    "# Helper function to get Nth trading day's close price\n",
    "def get_nth_day_close(ticker, start_date, n_days, price_data):\n",
    "    \"\"\"\n",
    "    Gets the close price on the Nth trading day *from and including* start_date.\n",
    "    n_days = 1 means close on start_date.\n",
    "    n_days = 2 means close on start_date + 1 trading day.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get all trading days for the ticker on or after start_date\n",
    "        ticker_prices_after_start = price_data.loc[(slice(start_date, None), ticker), 'Close']\n",
    "        if len(ticker_prices_after_start) >= n_days:\n",
    "            return ticker_prices_after_start.iloc[n_days - 1] # 0-indexed\n",
    "        else:\n",
    "            return np.nan # Not enough trading days\n",
    "    except KeyError:\n",
    "        return np.nan # Ticker or start date not found\n",
    "    except Exception: # Catch any other indexing error\n",
    "        return np.nan\n",
    "\n",
    "for index, event_row in df_events_cleaned.iterrows():\n",
    "    ticker = event_row['Normalized_Ticker']\n",
    "    entry_date_strat1 = event_row['Entry_Date_Strat1'] # This is the date of the Open price for momentum calc\n",
    "    entry_price_open = event_row['Entry_Price_Strat1'] # Open price on Entry_Date_Strat1\n",
    "\n",
    "    if pd.isna(entry_date_strat1) or pd.isna(entry_price_open) or entry_price_open == 0:\n",
    "        continue\n",
    "\n",
    "    # Calculate Initial 2-Day Return\n",
    "    # (Close on Entry_Date_Strat1 + 1 trading day) / Open on Entry_Date_Strat1 - 1\n",
    "    close_price_day2 = get_nth_day_close(ticker, entry_date_strat1, 2, stock_price_data_multi)\n",
    "    if pd.notna(close_price_day2):\n",
    "        df_events_cleaned.loc[index, 'Initial_2Day_Return'] = (close_price_day2 - entry_price_open) / entry_price_open\n",
    "\n",
    "    # Calculate Initial 5-Day Return\n",
    "    # (Close on Entry_Date_Strat1 + 4 trading days) / Open on Entry_Date_Strat1 - 1\n",
    "    close_price_day5 = get_nth_day_close(ticker, entry_date_strat1, 5, stock_price_data_multi)\n",
    "    if pd.notna(close_price_day5):\n",
    "        df_events_cleaned.loc[index, 'Initial_5Day_Return'] = (close_price_day5 - entry_price_open) / entry_price_open\n",
    "\n",
    "# --- Scatter Plots and Correlations ---\n",
    "momentum_indicators = {\n",
    "    'Initial_Day_Return': 'Initial 1-Day Return', # From previous step, assuming column exists\n",
    "    'Initial_2Day_Return': 'Initial 2-Day Return',\n",
    "    'Initial_5Day_Return': 'Initial 5-Day Return'\n",
    "}\n",
    "\n",
    "if 'Initial_Day_Return' not in df_events_cleaned.columns:\n",
    "    print(\"'Initial_Day_Return' column not found. Please ensure previous script was run or uncomment calculation.\")\n",
    "    # Potentially re-run calculation for Initial_Day_Return if needed, or remove from dict\n",
    "\n",
    "for col_name, plot_label in momentum_indicators.items():\n",
    "    if col_name not in df_events_cleaned.columns: # Skip if a column wasn't created (e.g. Initial_Day_Return missing)\n",
    "        print(f\"Skipping {plot_label} as column {col_name} is not in DataFrame.\")\n",
    "        continue\n",
    "\n",
    "    plot_df = df_events_cleaned[[col_name, 'Pct_Change_Strat1']].dropna()\n",
    "\n",
    "    if not plot_df.empty and len(plot_df) > 1: # Need at least 2 points for correlation\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.scatterplot(x=col_name, y='Pct_Change_Strat1', data=plot_df, alpha=0.6, label=f\"N={len(plot_df)}\")\n",
    "        sns.regplot(x=col_name, y='Pct_Change_Strat1', data=plot_df, scatter=False, color='red', line_kws={'label':\"Linear Reg.\"})\n",
    "\n",
    "        plt.title(f'Overall Drift vs. {plot_label}', fontsize=16)\n",
    "        plt.xlabel(f'{plot_label} (from Open on Entry_Date_Strat1)', fontsize=12)\n",
    "        plt.ylabel('Overall Pct Change (Entry to Effective Date)', fontsize=12)\n",
    "        plt.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "        plt.axvline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        correlation, p_value = pearsonr(plot_df[col_name], plot_df['Pct_Change_Strat1'])\n",
    "        print(f\"\\n--- {plot_label} ---\")\n",
    "        print(f\"Pearson Correlation with Pct_Change_Strat1: {correlation:.4f}\")\n",
    "        print(f\"P-value: {p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(\"The correlation is statistically significant at the 0.05 level.\")\n",
    "        else:\n",
    "            print(\"The correlation is not statistically significant at the 0.05 level.\")\n",
    "        print(f\"Number of valid data points for this analysis: {len(plot_df)}\")\n",
    "        print(\"Descriptive statistics for this indicator:\")\n",
    "        print(df_events_cleaned[col_name].describe())\n",
    "    else:\n",
    "        print(f\"\\nNot enough data to analyze {plot_label} after dropping NaNs (available: {len(plot_df)}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 500\n",
    "df_events_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from pandas.tseries.offsets import BDay # Business Day offset\n",
    "\n",
    "\n",
    "print(\"Calculating evolving correlation of 1-day test returns with overall drift...\")\n",
    "\n",
    "# Define the range of offsets from Announce_Date (in TRADING DAYS)\n",
    "# Negative offsets are days before announcement, positive are after.\n",
    "# Offset 0 would be the Announce_Date itself.\n",
    "# Offset +1 would be the first trading day after Announce_Date.\n",
    "day_offsets = list(range(-5, 11)) # From 5 trading days before to 10 trading days after\n",
    "\n",
    "correlations = []\n",
    "p_values = []\n",
    "num_observations = []\n",
    "actual_offsets_used = [] # To store the offset if calculation was possible\n",
    "\n",
    "# Ensure stock_price_data_multi has 'Close' and is sorted\n",
    "stock_price_data_multi.sort_index(inplace=True)\n",
    "\n",
    "# Pre-calculate 1-day returns for all stocks to speed up lookups\n",
    "# Using 'Adj Close' is often preferred for returns to handle splits/dividends,\n",
    "# but the project implies using Open/Close. Let's stick to 'Close' for consistency with prev steps unless specified.\n",
    "# If 'Adj Close' is available and reliable, it's generally better for return calculations over time.\n",
    "# For this, let's use 'Close' as per previous work.\n",
    "if 'Daily_Return' not in stock_price_data_multi.columns:\n",
    "    stock_price_data_multi['Daily_Return'] = stock_price_data_multi.groupby(level='Ticker')['Close'].transform(\n",
    "        lambda x: x.pct_change()\n",
    "    )\n",
    "\n",
    "for offset_days in day_offsets:\n",
    "    test_returns_for_offset = []\n",
    "    target_returns_for_offset = [] # Pct_Change_Strat1 for corresponding events\n",
    "\n",
    "    for index, event_row in df_events_cleaned.iterrows():\n",
    "        ticker = event_row['Normalized_Ticker']\n",
    "        announce_date = event_row['Announce_Date']\n",
    "        entry_price_strat1 = event_row['Entry_Price_Strat1']\n",
    "        pct_change_strat1 = event_row['Pct_Change_Strat1']\n",
    "        effective_date = event_row['Trade_Date']\n",
    "        entry_date_strat1 = event_row['Entry_Date_Strat1'] # Day after announce date\n",
    "\n",
    "        if pd.isna(pct_change_strat1):\n",
    "            continue\n",
    "\n",
    "        # Determine the actual \"Signal Calculation Day\"\n",
    "        # This requires finding the Nth business day from announce_date\n",
    "        # We need to find the trading day that corresponds to Announce_Date + offset_days\n",
    "\n",
    "        # Get all trading days for the ticker\n",
    "        try:\n",
    "            ticker_trading_days = stock_price_data_multi.loc[(slice(None), ticker), :].index.get_level_values('Date').unique().sort_values()\n",
    "        except KeyError:\n",
    "            continue # Ticker not in price data\n",
    "\n",
    "        if ticker_trading_days.empty:\n",
    "            continue\n",
    "\n",
    "        # Find Announce_Date's position or closest if not a trading day\n",
    "        # For simplicity, let's assume Announce_Date itself IS a trading day, or we align to it.\n",
    "        # A robust way: find the index of Announce_Date, then add offset.\n",
    "        try:\n",
    "            # Find the trading day on or immediately after Announce_Date\n",
    "            actual_announce_td = ticker_trading_days[ticker_trading_days >= announce_date][0]\n",
    "            announce_date_idx_in_ticker_days = ticker_trading_days.get_loc(actual_announce_td)\n",
    "        except (IndexError, KeyError): # Announce_Date is after last trading day for ticker, or other issue\n",
    "            continue\n",
    "\n",
    "        signal_calc_day_idx = announce_date_idx_in_ticker_days + offset_days\n",
    "\n",
    "        if 0 <= signal_calc_day_idx < len(ticker_trading_days):\n",
    "            signal_calc_day = ticker_trading_days[signal_calc_day_idx]\n",
    "\n",
    "            # Ensure signal_calc_day is before the period of Pct_Change_Strat1 starts\n",
    "            # Pct_Change_Strat1 starts effectively on entry_date_strat1.\n",
    "            # And also ensure signal_calc_day is not too close to or after effective_date\n",
    "            if signal_calc_day >= entry_date_strat1 and offset_days < 1: # Test return should not overlap/be after start of target return period unless offset is positive\n",
    "                 # If offset_days is 0 or negative, signal_calc_day should be <= announce_date.\n",
    "                 # entry_date_strat1 is announce_date + 1BD (approx).\n",
    "                 # So if signal_calc_day is on or after entry_date_strat1 for negative/zero offsets, there's an issue.\n",
    "                 # This check might be too strict or needs refinement based on exact definitions.\n",
    "                 # Let's assume for now that we want the signal day to be distinct and generally prior or at start.\n",
    "                 pass # Allow positive offsets to be within the Pct_Change_Strat1 window\n",
    "\n",
    "            if signal_calc_day >= effective_date: # Signal calc day should not be after the target period ends\n",
    "                continue\n",
    "\n",
    "            # Get the 1-day test return ending on signal_calc_day\n",
    "            try:\n",
    "                test_return = (stock_price_data_multi.loc[(signal_calc_day, ticker), 'Close'] - entry_price_strat1)/entry_price_strat1\n",
    "                if pd.notna(test_return):\n",
    "                    test_returns_for_offset.append(test_return)\n",
    "                    target_returns_for_offset.append(pct_change_strat1)\n",
    "            except KeyError:\n",
    "                continue # Data not available for this specific day/ticker\n",
    "        else:\n",
    "            # Offset leads to a date outside the ticker's trading history\n",
    "            continue\n",
    "\n",
    "    if len(test_returns_for_offset) > 1: # Need at least 2 pairs for correlation\n",
    "        corr, p_val = pearsonr(test_returns_for_offset, target_returns_for_offset)\n",
    "        correlations.append(corr)\n",
    "        p_values.append(p_val)\n",
    "        num_observations.append(len(test_returns_for_offset))\n",
    "        actual_offsets_used.append(offset_days)\n",
    "    else:\n",
    "        # Not enough data for this offset\n",
    "        # To keep plot x-axis consistent, we can append NaN or skip\n",
    "        pass # The actual_offsets_used will reflect which ones we got data for\n",
    "\n",
    "# --- Plotting the results ---\n",
    "if actual_offsets_used:\n",
    "    results_df = pd.DataFrame({\n",
    "        'Offset_Days': actual_offsets_used,\n",
    "        'Correlation': correlations,\n",
    "        'P_Value': p_values,\n",
    "        'N_Observations': num_observations\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Plot Correlation\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.lineplot(x='Offset_Days', y='Correlation', data=results_df, marker='o', label='Pearson Correlation')\n",
    "    plt.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    plt.axvline(0, color='red', linestyle=':', linewidth=0.8, label='Announcement Day (Offset=0)')\n",
    "    plt.axvline(1, color='green', linestyle=':', linewidth=0.8, label='Entry Day for Strat1 (Offset=+1 TD)')\n",
    "    plt.title('Evolution of Correlation: 1-Day Test Return vs. Overall Event Drift', fontsize=16)\n",
    "    plt.xlabel('Signal Calculation Day Offset from Announcement (Trading Days)', fontsize=12)\n",
    "    plt.ylabel('Pearson Correlation', fontsize=12)\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Number of Observations\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.barplot(x='Offset_Days', y='N_Observations', data=results_df, color='skyblue')\n",
    "    plt.title('Number of Observations for Each Offset Day', fontsize=14)\n",
    "    plt.xlabel('Signal Calculation Day Offset from Announcement (Trading Days)', fontsize=12)\n",
    "    plt.ylabel('Number of Events (N)', fontsize=12)\n",
    "    plt.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nCorrelation Results by Offset Day:\")\n",
    "    print(results_df)\n",
    "else:\n",
    "    print(\"No correlation data generated. Check data availability or offset range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc020e",
   "metadata": {},
   "source": [
    "might wanna check how signal changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967edd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "\n",
    "print(\"Calculating Total_Event_Trading_Days for each event...\")\n",
    "\n",
    "df_events_cleaned['Total_Event_Trading_Days'] = np.nan\n",
    "\n",
    "for index, event_row in df_events_cleaned.iterrows():\n",
    "    ticker = event_row['Normalized_Ticker']\n",
    "    announce_date = event_row['Announce_Date']\n",
    "    effective_date = event_row['Trade_Date']\n",
    "\n",
    "    try:\n",
    "        # Get trading days for this ticker between announce and effective (inclusive for count)\n",
    "        ticker_event_period_prices = stock_price_data_multi.loc[(slice(announce_date, effective_date), ticker), :]\n",
    "        if not ticker_event_period_prices.empty:\n",
    "            # Number of unique trading days in the interval\n",
    "            num_trading_days = ticker_event_period_prices.index.get_level_values('Date').nunique()\n",
    "            df_events_cleaned.loc[index, 'Total_Event_Trading_Days'] = num_trading_days\n",
    "    except KeyError:\n",
    "        # Ticker or part of date range not in price data\n",
    "        # print(f\"Price data missing for {ticker} between {announce_date} and {effective_date}\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        # print(f\"Error calculating trading days for event index {index}: {e}\")\n",
    "        pass\n",
    "\n",
    "# --- Analyze the distribution of Total_Event_Trading_Days ---\n",
    "print(\"\\nDistribution of Total_Event_Trading_Days:\")\n",
    "print(df_events_cleaned['Total_Event_Trading_Days'].describe())\n",
    "print(\"\\nValue Counts for Total_Event_Trading_Days:\")\n",
    "print(df_events_cleaned['Total_Event_Trading_Days'].value_counts().sort_index())\n",
    "\n",
    "# Plot distribution of Total_Event_Trading_Days\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_events_cleaned['Total_Event_Trading_Days'].dropna(), discrete=True, kde=False)\n",
    "plt.title('Distribution of Total Event Trading Days (Announce to Effective)', fontsize=16)\n",
    "plt.xlabel('Number of Trading Days', fontsize=12)\n",
    "plt.ylabel('Frequency (Number of Events)', fontsize=12)\n",
    "plt.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "print(\"Generating evolving correlation plots for grouped Total_Event_Trading_Days...\")\n",
    "\n",
    "# Define the groups based on Total_Event_Trading_Days\n",
    "holding_period_groups = {\n",
    "    \"Short (2-6 days)\": (2.0, 6.0),\n",
    "    \"Long (8-11 days)\": (8.0, 11.0)\n",
    "}\n",
    "\n",
    "# Ensure stock_price_data_multi has 'Close' and 'Daily_Return' and is sorted\n",
    "stock_price_data_multi.sort_index(inplace=True)\n",
    "\n",
    "for group_name, (min_days, max_days) in holding_period_groups.items():\n",
    "    print(f\"\\n--- Analyzing for Group: {group_name} ---\")\n",
    "\n",
    "    # Filter events for this specific group\n",
    "    group_events_df = df_events_cleaned[\n",
    "        (df_events_cleaned['Total_Event_Trading_Days'] >= min_days) &\n",
    "        (df_events_cleaned['Total_Event_Trading_Days'] <= max_days)\n",
    "    ].copy()\n",
    "\n",
    "    if group_events_df.empty:\n",
    "        print(f\"No events found for group: {group_name}.\")\n",
    "        continue\n",
    "\n",
    "    # Determine the maximum signal offset to consider for this group.\n",
    "    # This should be based on the shortest event in the group to ensure comparability,\n",
    "    # or we can allow the x-axis to extend as far as the longest event in the group allows.\n",
    "    # Let's allow it to extend, and N will drop for shorter events within the group.\n",
    "    # Max possible offset is max_days_in_group - 2.\n",
    "    # We can analyze up to, say, 10 post-announcement trading days if data allows.\n",
    "    max_post_announce_offset_to_test = int(max_days) - 2 # Ensure at least 1 day for remaining drift for longest event in group\n",
    "                                                        # And signal day is 1 day after announce_date (index 0)\n",
    "\n",
    "    # Define the range of signal calculation days (number of trading days after Announce_Date)\n",
    "    # Offset_PATD1 = 0 -> 1st trading day after Announce_Date\n",
    "    # Offset_PATD1 = 1 -> 2nd trading day after Announce_Date\n",
    "    # ...\n",
    "    # Max offset_patd1 is such that Signal_Calc_Day_T is one day before Effective_Date of the *shortest* event\n",
    "    # For a group, let's test up to a reasonable max, e.g., 9, and let N handle availability.\n",
    "    # If max_days is 11, max_offset_patd1 is 11-2 = 9. (Signal on 10th day post announce, drift on 11th day)\n",
    "\n",
    "    post_announce_offsets = list(range(0, int(max_days) - 1)) # from 0 up to max_days-2 (inclusive for offset_patd1)\n",
    "\n",
    "    correlations_group = []\n",
    "    p_values_group = []\n",
    "    num_observations_group = []\n",
    "    actual_offsets_plot = [] # The Signal_Day_Num_Post_Announce (offset_patd1 + 1)\n",
    "\n",
    "    for offset_patd1 in post_announce_offsets: # 0 means 1st day after announce, 1 means 2nd, etc.\n",
    "        test_returns_for_offset = []\n",
    "        target_returns_for_offset = []\n",
    "\n",
    "        current_signal_day_num = offset_patd1 # Day 1, Day 2, ... post announcement\n",
    "\n",
    "        for index, event_row in group_events_df.iterrows():\n",
    "            ticker = event_row['Normalized_Ticker']\n",
    "            announce_date = event_row['Announce_Date']\n",
    "            effective_date = event_row['Trade_Date']\n",
    "            event_total_days = event_row['Total_Event_Trading_Days']\n",
    "            pct_change_strat1 = event_row['Pct_Change_Strat1']\n",
    "            exit_price_strat1 = event_row['Exit_Price_Strat1']\n",
    "            entry_price_strat1 = event_row['Entry_Price_Strat1']\n",
    "\n",
    "\n",
    "            # Ensure this event is long enough for the current offset_patd1\n",
    "            if current_signal_day_num >= event_total_days: # Signal day must be < total days (i.e. before effective date)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ticker_trading_days = stock_price_data_multi.loc[(slice(None), ticker), :].index.get_level_values('Date').unique().sort_values()\n",
    "                actual_announce_td_idx = ticker_trading_days.get_loc(ticker_trading_days[ticker_trading_days >= announce_date][0])\n",
    "            except (KeyError, IndexError): continue\n",
    "\n",
    "            signal_calc_day_idx = actual_announce_td_idx + current_signal_day_num # current_signal_day_num is offset_patd1 + 1\n",
    "\n",
    "            if not (0 <= signal_calc_day_idx < len(ticker_trading_days)): continue\n",
    "            signal_calc_day_t = ticker_trading_days[signal_calc_day_idx]\n",
    "\n",
    "            if signal_calc_day_t >= effective_date: continue # Should be caught by above check too\n",
    "\n",
    "            #test_return = stock_price_data_multi.loc[(signal_calc_day_t, ticker), 'Daily_Return']\n",
    "            test_return = (stock_price_data_multi.loc[(signal_calc_day_t, ticker), 'Close'] - entry_price_strat1)/entry_price_strat1\n",
    "            print(\"ticker\" + str(ticker))\n",
    "            print(stock_price_data_multi.loc[(slice(announce_date, effective_date), ticker), :]) #slice(announce_date + pd.Timedelta(days=1), None)\n",
    "            print(\"price on signal day\" + str(stock_price_data_multi.loc[(signal_calc_day_t, ticker), 'Close']))\n",
    "            print(\"price on first day\" + str(entry_price_strat1))\n",
    "            print(\"test return is\" + str(test_return))\n",
    "            close_signal_calc_day_t = stock_price_data_multi.loc[(signal_calc_day_t, ticker), 'Close']\n",
    "\n",
    "            try:\n",
    "                actual_effective_date_td = ticker_trading_days[ticker_trading_days <= effective_date][-1]\n",
    "                if actual_effective_date_td <= signal_calc_day_t: continue\n",
    "                close_effective_date = stock_price_data_multi.loc[(actual_effective_date_td, ticker), 'Close']\n",
    "            except (KeyError, IndexError): continue\n",
    "\n",
    "            if pd.notna(test_return) and pd.notna(close_signal_calc_day_t) and close_signal_calc_day_t != 0 and pd.notna(close_effective_date):\n",
    "                remaining_drift = (exit_price_strat1 - stock_price_data_multi.loc[(signal_calc_day_t, ticker), 'Close'])/stock_price_data_multi.loc[(signal_calc_day_t, ticker), 'Close']\n",
    "                test_returns_for_offset.append(test_return)\n",
    "                print(test_return)\n",
    "                target_returns_for_offset.append(pct_change_strat1)\n",
    "                print(pct_change_strat1)\n",
    "\n",
    "        if len(test_returns_for_offset) > 1:\n",
    "            corr, p_val = pearsonr(test_returns_for_offset, target_returns_for_offset)\n",
    "            correlations_group.append(corr)\n",
    "            p_values_group.append(p_val)\n",
    "            num_observations_group.append(len(test_returns_for_offset))\n",
    "            actual_offsets_plot.append(current_signal_day_num)\n",
    "\n",
    "    if actual_offsets_plot:\n",
    "        group_results_df = pd.DataFrame({\n",
    "            'Signal_Day_Num_Post_Announce': actual_offsets_plot,\n",
    "            'Correlation': correlations_group,\n",
    "            'P_Value': p_values_group,\n",
    "            'N_Observations': num_observations_group\n",
    "        })\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "        fig.suptitle(f'Evolving Correlation for Holding Period Group: {group_name} (N_events_in_group={len(group_events_df)})', fontsize=16)\n",
    "\n",
    "        sns.lineplot(x='Signal_Day_Num_Post_Announce', y='Correlation', data=group_results_df, marker='o', ax=axes[0])\n",
    "        axes[0].axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "        axes[0].set_title('Correlation: X-Day Pct Return vs. Total Effective Date Return')\n",
    "        axes[0].set_ylabel('Pearson Correlation')\n",
    "        axes[0].grid(True, linestyle=':', alpha=0.7)\n",
    "\n",
    "        sns.barplot(x='Signal_Day_Num_Post_Announce', y='N_Observations', data=group_results_df, color='skyblue', ax=axes[1])\n",
    "        axes[1].set_title('Number of Observations for Each Signal Day')\n",
    "        axes[1].set_xlabel('Signal Calculation Day Number (Trading Days After Announcement)')\n",
    "        axes[1].set_ylabel('Number of Paired Observations (N)')\n",
    "        axes[1].grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "        plt.xticks(np.unique(actual_offsets_plot)) # Ensure all actual offset days are shown as ticks\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Correlation Results for Group: {group_name}:\")\n",
    "        print(group_results_df)\n",
    "    else:\n",
    "        print(f\"No correlation data generated for group: {group_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eabf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates # For date formatting on colorbar\n",
    "\n",
    "\n",
    "momentum_indicators = {\n",
    "    'Initial_Day_Return': 'Initial 1-Day Return',\n",
    "    'Initial_2Day_Return': 'Initial 2-Day Return',\n",
    "    'Initial_5Day_Return': 'Initial 5-Day Return'\n",
    "}\n",
    "\n",
    "df_events_cleaned['Announce_Date'] = pd.to_datetime(df_events_cleaned['Announce_Date'])\n",
    "\n",
    "for col_name, plot_label in momentum_indicators.items():\n",
    "    if col_name not in df_events_cleaned.columns:\n",
    "        print(f\"Skipping {plot_label} as column {col_name} is not in DataFrame.\")\n",
    "        continue\n",
    "\n",
    "    required_cols_for_plot = [col_name, 'Pct_Change_Strat1', 'Announce_Date']\n",
    "    if not all(c in df_events_cleaned.columns for c in required_cols_for_plot):\n",
    "        print(f\"Skipping {plot_label} due to missing one of required columns: {required_cols_for_plot}\")\n",
    "        continue\n",
    "\n",
    "    plot_df = df_events_cleaned[required_cols_for_plot].dropna().copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Convert Announce_Date to a numerical representation for hue\n",
    "    # Using to_julian_date() which gives a float, or .map(pd.Timestamp.toordinal) for integer ordinals\n",
    "    # Matplotlib's mdates.date2num also works well.\n",
    "    plot_df['Announce_Date_Num'] = plot_df['Announce_Date'].map(mdates.date2num)\n",
    "\n",
    "\n",
    "    if not plot_df.empty and len(plot_df) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(13, 8)) # Get explicit figure and axes objects\n",
    "\n",
    "        # Scatterplot with hue using the numerical date\n",
    "        # The `hue_norm` parameter can be used with a tuple (min, max) to set the normalization range for the hue.\n",
    "        # This helps ensure the color mapping is consistent if you plot multiple times or have outliers.\n",
    "        norm = plt.Normalize(plot_df['Announce_Date_Num'].min(), plot_df['Announce_Date_Num'].max())\n",
    "        sm = plt.cm.ScalarMappable(cmap=\"mako_r\", norm=norm)\n",
    "        sm.set_array([]) # You need to set_array for the ScalarMappable to work correctly with colorbar\n",
    "\n",
    "        points = ax.scatter(\n",
    "            plot_df[col_name],\n",
    "            plot_df['Pct_Change_Strat1'],\n",
    "            c=plot_df['Announce_Date_Num'], # Use numerical date for color\n",
    "            cmap='mako_r',                  # Colormap\n",
    "            norm=norm,                      # Normalize colors based on date range\n",
    "            alpha=0.7,\n",
    "            s=50\n",
    "        )\n",
    "\n",
    "        # Add regression line (plotted on the same axes)\n",
    "        sns.regplot(\n",
    "            x=col_name,\n",
    "            y='Pct_Change_Strat1',\n",
    "            data=plot_df,\n",
    "            scatter=False,\n",
    "            color='red',\n",
    "            ax=ax, # Explicitly pass the axes\n",
    "            label=\"Linear Reg. (All Data)\"\n",
    "        )\n",
    "\n",
    "        ax.set_title(f'Overall Drift vs. {plot_label} (Hued by Event Recency)', fontsize=16)\n",
    "        ax.set_xlabel(f'{plot_label} (from Open on Entry_Date_Strat1)', fontsize=12)\n",
    "        ax.set_ylabel('Overall Pct Change (Entry to Effective Date)', fontsize=12)\n",
    "        ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "        ax.axvline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "        ax.grid(True, linestyle=':', alpha=0.7)\n",
    "\n",
    "        # Add the colorbar\n",
    "        cbar = fig.colorbar(points, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Announcement Date Recency', fontsize=10)\n",
    "\n",
    "        # Format colorbar ticks as dates\n",
    "        # Get current ticks which are numerical dates\n",
    "        cbar_ticks_num = cbar.get_ticks()\n",
    "        # Convert numerical dates back to datetime objects for formatting\n",
    "        cbar_tick_dates = [mdates.num2date(tick_val) for tick_val in cbar_ticks_num]\n",
    "        # Format them. You can adjust the format string as needed.\n",
    "        cbar.set_ticklabels([date_obj.strftime('%Y-%m-%d') for date_obj in cbar_tick_dates])\n",
    "\n",
    "        # Add legend for the regression line\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        correlation, p_value = pearsonr(plot_df[col_name], plot_df['Pct_Change_Strat1'])\n",
    "        print(f\"\\n--- {plot_label} (Overall Correlation) ---\")\n",
    "        # ... (rest of the print statements are the same)\n",
    "        print(f\"Pearson Correlation with Pct_Change_Strat1: {correlation:.4f}\")\n",
    "        print(f\"P-value: {p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(\"The correlation is statistically significant at the 0.05 level.\")\n",
    "        else:\n",
    "            print(\"The correlation is not statistically significant at the 0.05 level.\")\n",
    "        print(f\"Number of valid data points for this analysis: {len(plot_df)}\")\n",
    "        print(f\"Descriptive statistics for {col_name} (plotted data):\")\n",
    "        print(plot_df[col_name].describe())\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nNot enough data to analyze {plot_label} after dropping NaNs (available: {len(plot_df)}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_events_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm # For detailed regression statistics\n",
    "regression_df = df_events_cleaned[['Initial_Day_Return', 'Pct_Change_Strat1', 'Announce_Date']].dropna().copy()\n",
    "\n",
    "if regression_df.empty or len(regression_df) < 2:\n",
    "    print(\"Not enough valid data points to fit a linear regression model after dropping NaNs.\")\n",
    "else:\n",
    "    Y = regression_df['Pct_Change_Strat1']\n",
    "    X = regression_df['Initial_Day_Return']\n",
    "    X_with_const = sm.add_constant(X) # Add intercept term\n",
    "\n",
    "    # 2. Fit the OLS model\n",
    "    model = sm.OLS(Y, X_with_const)\n",
    "    results = model.fit()\n",
    "\n",
    "    # 3. Print the model summary\n",
    "    print(\"\\n--- Linear Regression Model Summary ---\")\n",
    "    print(results.summary())\n",
    "\n",
    "    intercept_val = results.params['const']\n",
    "    slope_val = results.params['Initial_Day_Return']\n",
    "    r_squared_val = results.rsquared\n",
    "\n",
    "    print(f\"\\nFitted Model Equation: Expected_Pct_Change_Strat1 = {intercept_val:.4f} + ({slope_val:.4f} * Initial_Day_Return)\")\n",
    "    print(f\"R-squared: {r_squared_val:.4f}\")\n",
    "\n",
    "    # 4. Visualize the Fit (using the colorbar approach for recency)\n",
    "    regression_df['Announce_Date_Num'] = regression_df['Announce_Date'].map(mdates.date2num)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 8))\n",
    "\n",
    "    norm = plt.Normalize(regression_df['Announce_Date_Num'].min(), regression_df['Announce_Date_Num'].max())\n",
    "\n",
    "    points = ax.scatter(\n",
    "        regression_df['Initial_Day_Return'],\n",
    "        regression_df['Pct_Change_Strat1'],\n",
    "        c=regression_df['Announce_Date_Num'],\n",
    "        cmap='mako_r',\n",
    "        norm=norm,\n",
    "        alpha=0.6,\n",
    "        s=50,\n",
    "        label='Actual Data Points' # Label for scatter points (though legend might be complex with hue)\n",
    "    )\n",
    "\n",
    "    # Plot the regression line from the model\n",
    "    # Generate x-values for the line covering the range of Initial_Day_Return\n",
    "    x_line_fit = np.array([regression_df['Initial_Day_Return'].min(), regression_df['Initial_Day_Return'].max()])\n",
    "    y_line_fit = results.predict(sm.add_constant(pd.DataFrame({'Initial_Day_Return': x_line_fit}))) # Use model.predict\n",
    "\n",
    "    ax.plot(x_line_fit, y_line_fit, color='red', linewidth=2, label=f'Regression Line (R²={r_squared_val:.2f})')\n",
    "\n",
    "    ax.set_title('Linear Regression: Pct_Change_Strat1 vs. Initial_Day_Return (Hued by Recency)', fontsize=16)\n",
    "    ax.set_xlabel('Initial Day Return (Day After Announcement, Open to Close)', fontsize=12)\n",
    "    ax.set_ylabel('Overall Pct Change (Entry to Effective Date)', fontsize=12)\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.axvline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.grid(True, linestyle=':', alpha=0.7)\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = fig.colorbar(points, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Announcement Date Recency', fontsize=10)\n",
    "    cbar_ticks_num = cbar.get_ticks()\n",
    "    cbar_tick_dates = [mdates.num2date(tick_val) for tick_val in cbar_ticks_num]\n",
    "    cbar.set_ticklabels([date_obj.strftime('%Y-%m-%d') for date_obj in cbar_tick_dates])\n",
    "\n",
    "    ax.legend(loc='best') # Show legend for regression line and potentially scatter if labeled\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nModel Interpretation Hints:\")\n",
    "    print(f\"- Intercept ({intercept_val:.4f}): Predicted Pct_Change_Strat1 if Initial_Day_Return is 0.\")\n",
    "    print(f\"- Slope ({slope_val:.4f}): For each 1% (0.01) increase in Initial_Day_Return, Pct_Change_Strat1 is predicted to change by {slope_val*0.01:.4f} (or {slope_val:.2f}%).\")\n",
    "    print(f\"- R-squared ({r_squared_val:.4f}): About {r_squared_val*100:.2f}% of the variation in Pct_Change_Strat1 can be explained by Initial_Day_Return.\")\n",
    "    print(\"- P>|t| for Initial_Day_Return: If < 0.05, the Initial_Day_Return is a statistically significant predictor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481056a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm # For detailed regression statistics\n",
    "regression_df = df_events_cleaned[['Initial_Day_Return', 'Pct_Change_Strat1', 'Announce_Date']].dropna().copy()\n",
    "\n",
    "if regression_df.empty or len(regression_df) < 2:\n",
    "    print(\"Not enough valid data points to fit a linear regression model after dropping NaNs.\")\n",
    "else:\n",
    "    Y = regression_df['Pct_Change_Strat1']\n",
    "    X = regression_df['Initial_Day_Return']\n",
    "    X_with_const = sm.add_constant(X) # Add intercept term\n",
    "\n",
    "    # 2. Fit the OLS model\n",
    "    model = sm.OLS(Y, X_with_const)\n",
    "    results = model.fit()\n",
    "\n",
    "    # 3. Print the model summary\n",
    "    print(\"\\n--- Linear Regression Model Summary ---\")\n",
    "    print(results.summary())\n",
    "\n",
    "    intercept_val = results.params['const']\n",
    "    slope_val = results.params['Initial_Day_Return']\n",
    "    r_squared_val = results.rsquared\n",
    "\n",
    "    print(f\"\\nFitted Model Equation: Expected_Pct_Change_Strat1 = {intercept_val:.4f} + ({slope_val:.4f} * Initial_Day_Return)\")\n",
    "    print(f\"R-squared: {r_squared_val:.4f}\")\n",
    "\n",
    "    # 4. Visualize the Fit (using the colorbar approach for recency)\n",
    "    regression_df['Announce_Date_Num'] = regression_df['Announce_Date'].map(mdates.date2num)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 8))\n",
    "\n",
    "    norm = plt.Normalize(regression_df['Announce_Date_Num'].min(), regression_df['Announce_Date_Num'].max())\n",
    "\n",
    "    points = ax.scatter(\n",
    "        regression_df['Initial_Day_Return'],\n",
    "        regression_df['Pct_Change_Strat1'],\n",
    "        c=regression_df['Announce_Date_Num'],\n",
    "        cmap='mako_r',\n",
    "        norm=norm,\n",
    "        alpha=0.6,\n",
    "        s=50,\n",
    "        label='Actual Data Points' # Label for scatter points (though legend might be complex with hue)\n",
    "    )\n",
    "\n",
    "    # Plot the regression line from the model\n",
    "    # Generate x-values for the line covering the range of Initial_Day_Return\n",
    "    x_line_fit = np.array([regression_df['Initial_Day_Return'].min(), regression_df['Initial_Day_Return'].max()])\n",
    "    y_line_fit = results.predict(sm.add_constant(pd.DataFrame({'Initial_Day_Return': x_line_fit}))) # Use model.predict\n",
    "\n",
    "    ax.plot(x_line_fit, y_line_fit, color='red', linewidth=2, label=f'Regression Line (R²={r_squared_val:.2f})')\n",
    "\n",
    "    ax.set_title('Linear Regression: Pct_Change_Strat1 vs. Initial_Day_Return (Hued by Recency)', fontsize=16)\n",
    "    ax.set_xlabel('Initial Day Return (Day After Announcement, Open to Close)', fontsize=12)\n",
    "    ax.set_ylabel('Overall Pct Change (Entry to Effective Date)', fontsize=12)\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.axvline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.grid(True, linestyle=':', alpha=0.7)\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = fig.colorbar(points, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Announcement Date Recency', fontsize=10)\n",
    "    cbar_ticks_num = cbar.get_ticks()\n",
    "    cbar_tick_dates = [mdates.num2date(tick_val) for tick_val in cbar_ticks_num]\n",
    "    cbar.set_ticklabels([date_obj.strftime('%Y-%m-%d') for date_obj in cbar_tick_dates])\n",
    "\n",
    "    ax.legend(loc='best') # Show legend for regression line and potentially scatter if labeled\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nModel Interpretation Hints:\")\n",
    "    print(f\"- Intercept ({intercept_val:.4f}): Predicted Pct_Change_Strat1 if Initial_Day_Return is 0.\")\n",
    "    print(f\"- Slope ({slope_val:.4f}): For each 1% (0.01) increase in Initial_Day_Return, Pct_Change_Strat1 is predicted to change by {slope_val*0.01:.4f} (or {slope_val:.2f}%).\")\n",
    "    print(f\"- R-squared ({r_squared_val:.4f}): About {r_squared_val*100:.2f}% of the variation in Pct_Change_Strat1 can be explained by Initial_Day_Return.\")\n",
    "    print(\"- P>|t| for Initial_Day_Return: If < 0.05, the Initial_Day_Return is a statistically significant predictor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_dimae(pct_change, di_mae):\n",
    "        if pct_change > 0:\n",
    "            return pct_change - di_mae\n",
    "        else:\n",
    "            return pct_change + di_mae\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Fitting Linear Regression Model: Pct_Change_Strat1 ~ Initial_Day_Return\")\n",
    "\n",
    "regression_df = df_events_cleaned[['Initial_Day_Return', 'Pct_Change_Strat1', 'Announce_Date']].dropna().copy()\n",
    "\n",
    "if regression_df.empty or len(regression_df) < 2:\n",
    "    print(\"Not enough valid data points to fit a linear regression model after dropping NaNs.\")\n",
    "else:\n",
    "    Y_actual = regression_df['Pct_Change_Strat1']\n",
    "    X_reg = regression_df['Initial_Day_Return']\n",
    "    X_with_const = sm.add_constant(X_reg)\n",
    "\n",
    "    model = sm.OLS(Y_actual, X_with_const)\n",
    "    results = model.fit()\n",
    "\n",
    "    print(\"\\n--- Linear Regression Model Summary ---\")\n",
    "    print(results.summary())\n",
    "    # ... (other summary printouts) ...\n",
    "\n",
    "    Y_predicted = results.predict(X_with_const)\n",
    "\n",
    "    df_events_cleaned['pred_pct_change1'] = np.nan\n",
    "    df_events_cleaned['pred_max_drawdown1'] = np.nan\n",
    "\n",
    "    # Use the index of regression_df_with_index to assign predictions to the correct rows in df_events_cleaned\n",
    "    df_events_cleaned.loc[regression_df.index, 'pred_pct_change1'] = Y_predicted\n",
    "\n",
    "    residuals_all = Y_actual - Y_predicted\n",
    "\n",
    "    # --- Identify Directionally Incorrect Predictions ---\n",
    "    # A prediction is directionally incorrect if sign(actual) * sign(predicted) == -1\n",
    "    # This handles cases where one is positive and the other is negative.\n",
    "    # It excludes cases where either actual or predicted is exactly zero if np.sign(0) is 0.\n",
    "    # Or, more explicitly: (Actual > 0 and Predicted < 0) OR (Actual < 0 and Predicted > 0)\n",
    "\n",
    "    # Let's define a small tolerance for \"zero\" to avoid issues with floating point comparisons\n",
    "    # if we wanted to include cases like Actual=0, Predicted > 0.\n",
    "    # For now, strict sign opposition is:\n",
    "    directionally_incorrect_mask = (np.sign(Y_actual) * np.sign(Y_predicted)) == -1\n",
    "\n",
    "    # Alternative explicit definition that handles 0 differently:\n",
    "    # directionally_incorrect_mask = (\n",
    "    #    ((Y_actual > 0) & (Y_predicted < 0)) |\n",
    "    #    ((Y_actual < 0) & (Y_predicted > 0))\n",
    "    # )\n",
    "    # Add these if you want to count predicting non-zero when actual is zero as directionally wrong:\n",
    "    # | ((Y_actual == 0) & (Y_predicted != 0))\n",
    "    # | ((Y_actual != 0) & (Y_predicted == 0))\n",
    "\n",
    "\n",
    "    Y_actual_di = Y_actual[directionally_incorrect_mask]\n",
    "    Y_predicted_di = Y_predicted[directionally_incorrect_mask]\n",
    "    residuals_di = residuals_all[directionally_incorrect_mask]\n",
    "\n",
    "    num_directionally_incorrect = directionally_incorrect_mask.sum()\n",
    "    total_predictions = len(Y_actual)\n",
    "\n",
    "    print(f\"\\n--- Directionally Incorrect Prediction Analysis ---\")\n",
    "    if num_directionally_incorrect > 0:\n",
    "        di_mae = mean_absolute_error(Y_actual_di, Y_predicted_di)\n",
    "        df_events_cleaned['pred_max_drawdown1'] = df_events_cleaned['pred_pct_change1'].apply(shift_dimae, args={di_mae})\n",
    "        print(f\"Number of Directionally Incorrect Predictions: {num_directionally_incorrect} out of {total_predictions} ({num_directionally_incorrect/total_predictions:.2%})\")\n",
    "        print(f\"Mean Absolute Error (MAE) for Directionally Incorrect Predictions (DI-MAE): {di_mae:.4f}\")\n",
    "\n",
    "        print(\"\\nResiduals for Directionally Incorrect Predictions:\")\n",
    "        print(f\"  Mean Residual: {residuals_di.mean():.4f}\")\n",
    "        print(f\"  Std Dev of Residuals: {residuals_di.std():.4f}\")\n",
    "        print(f\"  Min Residual: {residuals_di.min():.4f}\")\n",
    "        print(f\"  Max Residual: {residuals_di.max():.4f}\")\n",
    "        print(\"  Percentiles of Residuals (Directionally Incorrect):\")\n",
    "        print(f\"    25th: {np.percentile(residuals_di, 25):.4f}\")\n",
    "        print(f\"    50th (Median): {np.percentile(residuals_di, 50):.4f}\")\n",
    "        print(f\"    75th: {np.percentile(residuals_di, 75):.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals_di, kde=True, bins=20 if num_directionally_incorrect > 20 else 5)\n",
    "        plt.title('Distribution of Residuals (Only for Directionally Incorrect Predictions)', fontsize=16)\n",
    "        plt.xlabel('Residual Value (Actual - Predicted)', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.axvline(0, color='red', linestyle='--')\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No directionally incorrect predictions found with the current criteria.\")\n",
    "\n",
    "    # --- Overall Error Metrics (repeated for context, already calculated before) ---\n",
    "    mae_all = mean_absolute_error(Y_actual, Y_predicted)\n",
    "    max_abs_error_all = np.max(np.abs(residuals_all))\n",
    "    print(f\"\\n--- Overall Error Metrics (All Predictions for reference) ---\")\n",
    "    print(f\"Overall MAE: {mae_all:.4f}\")\n",
    "    print(f\"Overall Max Absolute Error: {max_abs_error_all:.4f}\")\n",
    "    # ... (RMSE, etc. from summary)\n",
    "\n",
    "    # ... (Scatter plot code remains the same as before) ...\n",
    "    # (Optional: You could highlight the directionally incorrect points on the scatter plot too)\n",
    "    regression_df['Announce_Date_Num'] = regression_df['Announce_Date'].map(mdates.date2num)\n",
    "    fig, ax = plt.subplots(figsize=(13, 8))\n",
    "    norm = plt.Normalize(regression_df['Announce_Date_Num'].min(), regression_df['Announce_Date_Num'].max())\n",
    "\n",
    "    # Plot all points first\n",
    "    points = ax.scatter(\n",
    "        regression_df['Initial_Day_Return'],\n",
    "        regression_df['Pct_Change_Strat1'],\n",
    "        c=regression_df['Announce_Date_Num'], cmap='mako_r', norm=norm, alpha=0.3, s=50, label='_nolegend_' # Make them fainter\n",
    "    )\n",
    "\n",
    "    # Highlight directionally incorrect points\n",
    "    if num_directionally_incorrect > 0:\n",
    "        ax.scatter(\n",
    "            X_reg[directionally_incorrect_mask], # X values for DI points\n",
    "            Y_actual[directionally_incorrect_mask],   # Y_actual values for DI points\n",
    "            color='red',\n",
    "            edgecolor='black',\n",
    "            s=70,\n",
    "            label=f'Directionally Incorrect ({num_directionally_incorrect})',\n",
    "            zorder=3 # Plot on top\n",
    "        )\n",
    "\n",
    "    x_line_fit = np.array([regression_df['Initial_Day_Return'].min(), regression_df['Initial_Day_Return'].max()])\n",
    "    y_line_fit = results.predict(sm.add_constant(pd.DataFrame({'Initial_Day_Return': x_line_fit})))\n",
    "    ax.plot(x_line_fit, y_line_fit, color='blue', linewidth=2, label=f'Regression Line (R²={results.rsquared:.2f})') # Changed line color for contrast\n",
    "    ax.set_title('Linear Regression (Highlighting Directionally Incorrect Predictions)', fontsize=16)\n",
    "    ax.set_xlabel('Initial Day Return', fontsize=12)\n",
    "    ax.set_ylabel('Overall Pct Change', fontsize=12)\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.axvline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.grid(True, linestyle=':', alpha=0.7)\n",
    "    cbar = fig.colorbar(points, ax=ax, orientation='vertical', fraction=0.046, pad=0.04) # Colorbar for original hue\n",
    "    cbar.set_label('Announcement Date Recency', fontsize=10)\n",
    "    cbar_ticks_num = cbar.get_ticks()\n",
    "    cbar_tick_dates = [mdates.num2date(tick_val) for tick_val in cbar_ticks_num]\n",
    "    cbar.set_ticklabels([date_obj.strftime('%Y-%m-%d') for date_obj in cbar_tick_dates])\n",
    "    ax.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55caf479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a38617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_cleaned['3_Day_Return'] = np.nan\n",
    "\n",
    "# Ensure stock_price_data_multi is sorted for the helper function\n",
    "if not stock_price_data_multi.index.is_monotonic_increasing:\n",
    "    print(\"asdfasf\")\n",
    "    stock_price_data_multi.sort_index(inplace=True)\n",
    "\n",
    "for index, event_row in df_events_cleaned.iterrows():\n",
    "    ticker = event_row['Normalized_Ticker']\n",
    "    # Entry_Date_Strat1 is the first day (Day 1) where we get the Open price\n",
    "    entry_date_day1 = event_row['Entry_Date_Strat1']\n",
    "    entry_price_open_day1 = event_row['Entry_Price_Strat1']\n",
    "\n",
    "    if pd.isna(entry_date_day1) or pd.isna(entry_price_open_day1) or entry_price_open_day1 == 0:\n",
    "        continue\n",
    "\n",
    "    # We need the close price of the 3rd trading day from and including entry_date_day1.\n",
    "    # So, n_trading_days_from_start = 3.\n",
    "    close_price_day3 = get_nth_day_close(ticker, entry_date_day1, 3, stock_price_data_multi)\n",
    "\n",
    "    if pd.notna(close_price_day3):\n",
    "        three_day_return = (close_price_day3 - entry_price_open_day1) / entry_price_open_day1\n",
    "        df_events_cleaned.loc[index, '3_Day_Return'] = three_day_return\n",
    "    #else:\n",
    "        # print(f\"Could not calculate 3_Day_Return for {ticker} on event index {index} (Entry Date: {entry_date_day1}). Close_price_day3 was NaN.\")\n",
    "\n",
    "\n",
    "print(\"\\nCalculation complete. '3_Day_Return' added to df_events_cleaned.\")\n",
    "print(\"Sample of df_events_cleaned with the new column:\")\n",
    "print(df_events_cleaned[['Normalized_Ticker', 'Entry_Date_Strat1', 'Entry_Price_Strat1', '3_Day_Return']].head())\n",
    "print(\"\\nDescriptive statistics for '3_Day_Return':\")\n",
    "print(df_events_cleaned['3_Day_Return'].describe())\n",
    "\n",
    "# Verify some calculations manually if using placeholder data or have small dataset\n",
    "# Example for first few non-NaN results:\n",
    "for i, row in df_events_cleaned.dropna(subset=['3_Day_Return']).head(2).iterrows():\n",
    "    print(f\"\\nVerification for event index {i}, Ticker: {row['Normalized_Ticker']}\")\n",
    "    print(f\"Entry_Date_Strat1 (Day 1): {row['Entry_Date_Strat1'].date()}, Open Price: {row['Entry_Price_Strat1']:.2f}\")\n",
    "    day1_data = stock_price_data_multi.loc[(row['Entry_Date_Strat1'], row['Normalized_Ticker'])]\n",
    "    print(f\"  Day 1 Close: {day1_data['Close']:.2f}\")\n",
    "\n",
    "# Find Day 2 and Day 3\n",
    "    ticker_days = stock_price_data_multi.loc[(slice(row['Entry_Date_Strat1'], row['Trade_Date']), row['Normalized_Ticker']), :].reset_index()\n",
    "    print(ticker_days)\n",
    "    if len(ticker_days) > 1:\n",
    "        day2_date = ticker_days.iloc[1]['Date']\n",
    "        print(day2_date)\n",
    "        day2_data = stock_price_data_multi.loc[(day2_date, row['Normalized_Ticker']),]\n",
    "        print(f\"  Day 2 ({day2_date.date()}) Close: {day2_data['Close']:.2f}\")\n",
    "    if len(ticker_days) > 2:\n",
    "        day3_date = ticker_days.iloc[2]['Date']\n",
    "        day3_data = stock_price_data_multi.loc[(day3_date, row['Normalized_Ticker'])]\n",
    "        print(f\"  Day 3 ({day3_date.date()}) Close: {day3_data['Close']:.2f} (Used for 3_Day_Return calc)\")\n",
    "print(f\"Calculated 3_Day_Return: {row['3_Day_Return']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4249713",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_df = df_events_cleaned[['3_Day_Return', 'Pct_Change_Strat1', 'Announce_Date']].dropna().copy()\n",
    "\n",
    "if regression_df.empty or len(regression_df) < 2:\n",
    "    print(\"Not enough valid data points to fit a linear regression model after dropping NaNs.\")\n",
    "else:\n",
    "    Y = regression_df['Pct_Change_Strat1']\n",
    "    X = regression_df['3_Day_Return']\n",
    "    X_with_const = sm.add_constant(X) # Add intercept term\n",
    "\n",
    "    # 2. Fit the OLS model\n",
    "    model = sm.OLS(Y, X_with_const)\n",
    "    results = model.fit()\n",
    "\n",
    "    # 3. Print the model summary\n",
    "    print(\"\\n--- Linear Regression Model Summary ---\")\n",
    "    print(results.summary())\n",
    "\n",
    "    intercept_val = results.params['const']\n",
    "    slope_val = results.params['3_Day_Return']\n",
    "    r_squared_val = results.rsquared\n",
    "\n",
    "    print(f\"\\nFitted Model Equation: Expected_Pct_Change_Strat1 = {intercept_val:.4f} + ({slope_val:.4f} * 3_Day_Return)\")\n",
    "    print(f\"R-squared: {r_squared_val:.4f}\")\n",
    "\n",
    "    # 4. Visualize the Fit (using the colorbar approach for recency)\n",
    "    regression_df['Announce_Date_Num'] = regression_df['Announce_Date'].map(mdates.date2num)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 8))\n",
    "\n",
    "    norm = plt.Normalize(regression_df['Announce_Date_Num'].min(), regression_df['Announce_Date_Num'].max())\n",
    "\n",
    "    points = ax.scatter(\n",
    "        regression_df['3_Day_Return'],\n",
    "        regression_df['Pct_Change_Strat1'],\n",
    "        c=regression_df['Announce_Date_Num'],\n",
    "        cmap='mako_r',\n",
    "        norm=norm,\n",
    "        alpha=0.6,\n",
    "        s=50,\n",
    "        label='Actual Data Points' # Label for scatter points (though legend might be complex with hue)\n",
    "    )\n",
    "\n",
    "    # Plot the regression line from the model\n",
    "    # Generate x-values for the line covering the range of Initial_Day_Return\n",
    "    x_line_fit = np.array([regression_df['3_Day_Return'].min(), regression_df['3_Day_Return'].max()])\n",
    "    y_line_fit = results.predict(sm.add_constant(pd.DataFrame({'3_Day_Return': x_line_fit}))) # Use model.predict\n",
    "\n",
    "    ax.plot(x_line_fit, y_line_fit, color='red', linewidth=2, label=f'Regression Line (R²={r_squared_val:.2f})')\n",
    "\n",
    "    ax.set_title('Linear Regression: Pct_Change_Strat1 vs. 3_Day_Return (Hued by Recency)', fontsize=16)\n",
    "    ax.set_xlabel('3 Day Return (Day After Announcement, Open to Close)', fontsize=12)\n",
    "    ax.set_ylabel('Overall Pct Change (Entry to Trade Date)', fontsize=12)\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.axvline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.grid(True, linestyle=':', alpha=0.7)\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = fig.colorbar(points, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Announcement Date Recency', fontsize=10)\n",
    "    cbar_ticks_num = cbar.get_ticks()\n",
    "    cbar_tick_dates = [mdates.num2date(tick_val) for tick_val in cbar_ticks_num]\n",
    "    cbar.set_ticklabels([date_obj.strftime('%Y-%m-%d') for date_obj in cbar_tick_dates])\n",
    "\n",
    "    ax.legend(loc='best') # Show legend for regression line and potentially scatter if labeled\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nModel Interpretation Hints:\")\n",
    "    print(f\"- Intercept ({intercept_val:.4f}): Predicted Pct_Change_Strat1 if 3_Day_Return is 0.\")\n",
    "    print(f\"- Slope ({slope_val:.4f}): For each 1% (0.01) increase in 3_Day_Return, Pct_Change_Strat1 is predicted to change by {slope_val*0.01:.4f} (or {slope_val:.2f}%).\")\n",
    "    print(f\"- R-squared ({r_squared_val:.4f}): About {r_squared_val*100:.2f}% of the variation in Pct_Change_Strat1 can be explained by Initial_Day_Return.\")\n",
    "    print(\"- P>|t| for 3_Day_Return: If < 0.05, the 3_Day_Return is a statistically significant predictor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Fitting Linear Regression Model: Pct_Change_Strat1 ~ Initial_Day_Return\")\n",
    "\n",
    "regression_df = df_events_cleaned[['3_Day_Return', 'Pct_Change_Strat1', 'Announce_Date']].dropna().copy()\n",
    "\n",
    "if regression_df.empty or len(regression_df) < 2:\n",
    "    print(\"Not enough valid data points to fit a linear regression model after dropping NaNs.\")\n",
    "else:\n",
    "    Y_actual = regression_df['Pct_Change_Strat1']\n",
    "    X_reg = regression_df['3_Day_Return']\n",
    "    X_with_const = sm.add_constant(X_reg)\n",
    "\n",
    "    model = sm.OLS(Y_actual, X_with_const)\n",
    "    results = model.fit()\n",
    "\n",
    "    print(\"\\n--- Linear Regression Model Summary ---\")\n",
    "    print(results.summary())\n",
    "    # ... (other summary printouts) ...\n",
    "\n",
    "    Y_predicted = results.predict(X_with_const)\n",
    "\n",
    "    df_events_cleaned['pred_pct_change3'] = np.nan\n",
    "    df_events_cleaned['pred_max_drawdown3'] = np.nan\n",
    "\n",
    "    # Use the index of regression_df_with_index to assign predictions to the correct rows in df_events_cleaned\n",
    "    df_events_cleaned.loc[regression_df.index, 'pred_pct_change3'] = Y_predicted\n",
    "\n",
    "    residuals_all = Y_actual - Y_predicted\n",
    "\n",
    "    # --- Identify Directionally Incorrect Predictions ---\n",
    "    # A prediction is directionally incorrect if sign(actual) * sign(predicted) == -1\n",
    "    # This handles cases where one is positive and the other is negative.\n",
    "    # It excludes cases where either actual or predicted is exactly zero if np.sign(0) is 0.\n",
    "    # Or, more explicitly: (Actual > 0 and Predicted < 0) OR (Actual < 0 and Predicted > 0)\n",
    "\n",
    "    # Let's define a small tolerance for \"zero\" to avoid issues with floating point comparisons\n",
    "    # if we wanted to include cases like Actual=0, Predicted > 0.\n",
    "    # For now, strict sign opposition is:\n",
    "    directionally_incorrect_mask = (np.sign(Y_actual) * np.sign(Y_predicted)) == -1\n",
    "\n",
    "    # Alternative explicit definition that handles 0 differently:\n",
    "    # directionally_incorrect_mask = (\n",
    "    #    ((Y_actual > 0) & (Y_predicted < 0)) |\n",
    "    #    ((Y_actual < 0) & (Y_predicted > 0))\n",
    "    # )\n",
    "    # Add these if you want to count predicting non-zero when actual is zero as directionally wrong:\n",
    "    # | ((Y_actual == 0) & (Y_predicted != 0))\n",
    "    # | ((Y_actual != 0) & (Y_predicted == 0))\n",
    "\n",
    "\n",
    "    Y_actual_di = Y_actual[directionally_incorrect_mask]\n",
    "    Y_predicted_di = Y_predicted[directionally_incorrect_mask]\n",
    "    residuals_di = residuals_all[directionally_incorrect_mask]\n",
    "\n",
    "    num_directionally_incorrect = directionally_incorrect_mask.sum()\n",
    "    total_predictions = len(Y_actual)\n",
    "\n",
    "    print(f\"\\n--- Directionally Incorrect Prediction Analysis ---\")\n",
    "    if num_directionally_incorrect > 0:\n",
    "        di_mae = mean_absolute_error(Y_actual_di, Y_predicted_di)\n",
    "        ###########################3333333\n",
    "        df_events_cleaned['pred_max_drawdown3'] = df_events_cleaned['pred_pct_change3'].apply(shift_dimae, args={di_mae})\n",
    "        print(f\"Number of Directionally Incorrect Predictions: {num_directionally_incorrect} out of {total_predictions} ({num_directionally_incorrect/total_predictions:.2%})\")\n",
    "        print(f\"Mean Absolute Error (MAE) for Directionally Incorrect Predictions (DI-MAE): {di_mae:.4f}\")\n",
    "\n",
    "        print(\"\\nResiduals for Directionally Incorrect Predictions:\")\n",
    "        print(f\"  Mean Residual: {residuals_di.mean():.4f}\")\n",
    "        print(f\"  Std Dev of Residuals: {residuals_di.std():.4f}\")\n",
    "        print(f\"  Min Residual: {residuals_di.min():.4f}\")\n",
    "        print(f\"  Max Residual: {residuals_di.max():.4f}\")\n",
    "        print(\"  Percentiles of Residuals (Directionally Incorrect):\")\n",
    "        print(f\"    25th: {np.percentile(residuals_di, 25):.4f}\")\n",
    "        print(f\"    50th (Median): {np.percentile(residuals_di, 50):.4f}\")\n",
    "        print(f\"    75th: {np.percentile(residuals_di, 75):.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals_di, kde=True, bins=20 if num_directionally_incorrect > 20 else 5)\n",
    "        plt.title('Distribution of Residuals (Only for Directionally Incorrect Predictions)', fontsize=16)\n",
    "        plt.xlabel('Residual Value (Actual - Predicted)', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.axvline(0, color='red', linestyle='--')\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No directionally incorrect predictions found with the current criteria.\")\n",
    "\n",
    "    # --- Overall Error Metrics (repeated for context, already calculated before) ---\n",
    "    mae_all = mean_absolute_error(Y_actual, Y_predicted)\n",
    "    max_abs_error_all = np.max(np.abs(residuals_all))\n",
    "    print(f\"\\n--- Overall Error Metrics (All Predictions for reference) ---\")\n",
    "    print(f\"Overall MAE: {mae_all:.4f}\")\n",
    "    print(f\"Overall Max Absolute Error: {max_abs_error_all:.4f}\")\n",
    "    # ... (RMSE, etc. from summary)\n",
    "\n",
    "    # ... (Scatter plot code remains the same as before) ...\n",
    "    # (Optional: You could highlight the directionally incorrect points on the scatter plot too)\n",
    "    regression_df['Announce_Date_Num'] = regression_df['Announce_Date'].map(mdates.date2num)\n",
    "    fig, ax = plt.subplots(figsize=(13, 8))\n",
    "    norm = plt.Normalize(regression_df['Announce_Date_Num'].min(), regression_df['Announce_Date_Num'].max())\n",
    "\n",
    "    # Plot all points first\n",
    "    points = ax.scatter(\n",
    "        regression_df['3_Day_Return'],\n",
    "        regression_df['Pct_Change_Strat1'],\n",
    "        c=regression_df['Announce_Date_Num'], cmap='mako_r', norm=norm, alpha=0.3, s=50, label='_nolegend_' # Make them fainter\n",
    "    )\n",
    "\n",
    "    # Highlight directionally incorrect points\n",
    "    if num_directionally_incorrect > 0:\n",
    "        ax.scatter(\n",
    "            X_reg[directionally_incorrect_mask], # X values for DI points\n",
    "            Y_actual[directionally_incorrect_mask],   # Y_actual values for DI points\n",
    "            color='red',\n",
    "            edgecolor='black',\n",
    "            s=70,\n",
    "            label=f'Directionally Incorrect ({num_directionally_incorrect})',\n",
    "            zorder=3 # Plot on top\n",
    "        )\n",
    "\n",
    "    x_line_fit = np.array([regression_df['3_Day_Return'].min(), regression_df['3_Day_Return'].max()])\n",
    "    y_line_fit = results.predict(sm.add_constant(pd.DataFrame({'3_Day_Return': x_line_fit})))\n",
    "    ax.plot(x_line_fit, y_line_fit, color='blue', linewidth=2, label=f'Regression Line (R²={results.rsquared:.2f})') # Changed line color for contrast\n",
    "    ax.set_title('Linear Regression (Highlighting Directionally Incorrect Predictions)', fontsize=16)\n",
    "    ax.set_xlabel('3 Day Return', fontsize=12)\n",
    "    ax.set_ylabel('Overall Pct Change', fontsize=12)\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.axvline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.grid(True, linestyle=':', alpha=0.7)\n",
    "    cbar = fig.colorbar(points, ax=ax, orientation='vertical', fraction=0.046, pad=0.04) # Colorbar for original hue\n",
    "    cbar.set_label('Announcement Date Recency', fontsize=10)\n",
    "    cbar_ticks_num = cbar.get_ticks()\n",
    "    cbar_tick_dates = [mdates.num2date(tick_val) for tick_val in cbar_ticks_num]\n",
    "    cbar.set_ticklabels([date_obj.strftime('%Y-%m-%d') for date_obj in cbar_tick_dates])\n",
    "    ax.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_events_cleaned))# Prepare event dates for signal generation and trading\n",
    "df_events_cleaned['Actual_Trade_Entry_Date_Initial'] = pd.NaT # Entry based on Initial_Day_Return\n",
    "df_events_cleaned['Resizing_Decision_Date'] = pd.NaT        # When 3_Day_Return is known\n",
    "df_events_cleaned['Actual_Resizing_Trade_Date'] = pd.NaT  # Day after Resizing_Decision_Date\n",
    "\n",
    "print(\"Preparing event dates for backtester (Initial Entry & Resizing)...\")\n",
    "for index, row in df_events_cleaned.iterrows():\n",
    "    entry_date_s1 = row['Entry_Date_Strat1'] # Date of Open for Initial_Day_Return calc\n",
    "    ticker = row['Normalized_Ticker']\n",
    "    try:\n",
    "        ticker_trading_days = stock_price_data_multi.loc[(slice(None), ticker), :].index.get_level_values('Date').unique().sort_values()\n",
    "\n",
    "        actual_entry_date_s1_td_query = ticker_trading_days[ticker_trading_days >= entry_date_s1]\n",
    "        if actual_entry_date_s1_td_query.empty: continue\n",
    "        actual_entry_date_s1_td = actual_entry_date_s1_td_query[0]\n",
    "        base_idx = ticker_trading_days.get_loc(actual_entry_date_s1_td) # Index of Entry_Date_Strat1\n",
    "\n",
    "        # Initial_Day_Return known at Close of base_idx (Entry_Date_Strat1)\n",
    "        # Initial Entry at Open of (base_idx + 1 trading days)\n",
    "        if base_idx + 1 < len(ticker_trading_days):\n",
    "            df_events_cleaned.loc[index, 'Actual_Trade_Entry_Date_Initial'] = ticker_trading_days[base_idx + 1]\n",
    "\n",
    "        # 3_Day_Return (Open Day1 to Close Day3) is known at Close of (base_idx + 2 trading days)\n",
    "        # Day1 is base_idx, Day2 is base_idx+1, Day3 is base_idx+2\n",
    "        if base_idx + 2 < len(ticker_trading_days):\n",
    "            df_events_cleaned.loc[index, 'Resizing_Decision_Date'] = ticker_trading_days[base_idx + 2]\n",
    "            # Actual resizing trade at Open of (base_idx + 3 trading days)\n",
    "            if base_idx + 3 < len(ticker_trading_days):\n",
    "                 df_events_cleaned.loc[index, 'Actual_Resizing_Trade_Date'] = ticker_trading_days[base_idx + 3]\n",
    "\n",
    "    except KeyError: continue\n",
    "    except Exception as e: print(f\"Error prepping event {index} for {ticker}: {e}\"); continue\n",
    "\n",
    "print(df_events_cleaned.dropna(subset=['Initial_Day_Return', 'pred_max_drawdown1', 'Actual_Trade_Entry_Date_Initial', 'Trade_Date']))\n",
    "\n",
    "mask_long_events = df_events_cleaned['Total_Event_Trading_Days'] > 6\n",
    "df_events_cleaned_long_needs_resize_dates = df_events_cleaned[mask_long_events].dropna(\n",
    "    subset=['3_Day_Return', 'pred_max_drawdown3', 'Resizing_Decision_Date', 'Actual_Resizing_Trade_Date']\n",
    ")\n",
    "\n",
    "print(\"resize\" + str(len(df_events_cleaned_long_needs_resize_dates)))\n",
    "\n",
    "df_events_cleaned_short_no_resize_dates = df_events_cleaned[~mask_long_events]\n",
    "\n",
    "print(\"no resize\" + str(len(df_events_cleaned_short_no_resize_dates)))\n",
    "df_events_cleaned = pd.concat([df_events_cleaned_short_no_resize_dates, df_events_cleaned_long_needs_resize_dates])\n",
    "\n",
    "print(df_events_cleaned)\n",
    "\n",
    "df_events_cleaned = df_events_cleaned[df_events_cleaned['Trade_Date'] >= df_events_cleaned['Actual_Trade_Entry_Date_Initial']].copy()\n",
    "\n",
    "print(\"aSDFASDF \" + str(len(df_events_cleaned)))\n",
    "\n",
    "df_events_cleaned.sort_values(by='Actual_Trade_Entry_Date_Initial', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration & Enhanced Data Preparation ---\n",
    "INITIAL_PORTFOLIO_VALUE = 5_000_000.00\n",
    "TRANSACTION_COST_PER_SHARE = 0.01\n",
    "LONG_FINANCING_SPREAD_ANNUAL = 0.015\n",
    "SHORT_FINANCING_SPREAD_ANNUAL = 0.010 # For short positions (cost to borrow, or rebate)\n",
    "RISK_LIMIT_PER_TRADE_VALUE = 250_000.00\n",
    "LIQUIDITY_ADV_PERCENT_CAP = 0.01\n",
    "\n",
    "# Prepare event dates for signal generation and trading\n",
    "df_events_cleaned['Actual_Trade_Entry_Date_Initial'] = pd.NaT # Entry based on Initial_Day_Return\n",
    "df_events_cleaned['Resizing_Decision_Date'] = pd.NaT        # When 3_Day_Return is known\n",
    "df_events_cleaned['Actual_Resizing_Trade_Date'] = pd.NaT  # Day after Resizing_Decision_Date\n",
    "\n",
    "print(\"Preparing event dates for backtester (Initial Entry & Resizing)...\")\n",
    "for index, row in df_events_cleaned.iterrows():\n",
    "    entry_date_s1 = row['Entry_Date_Strat1'] # Date of Open for Initial_Day_Return calc\n",
    "    ticker = row['Normalized_Ticker']\n",
    "    try:\n",
    "        ticker_trading_days = stock_price_data_multi.loc[(slice(None), ticker), :].index.get_level_values('Date').unique().sort_values()\n",
    "\n",
    "        actual_entry_date_s1_td_query = ticker_trading_days[ticker_trading_days >= entry_date_s1]\n",
    "        if actual_entry_date_s1_td_query.empty: continue\n",
    "        actual_entry_date_s1_td = actual_entry_date_s1_td_query[0]\n",
    "        base_idx = ticker_trading_days.get_loc(actual_entry_date_s1_td) # Index of Entry_Date_Strat1\n",
    "\n",
    "        # Initial_Day_Return known at Close of base_idx (Entry_Date_Strat1)\n",
    "        # Initial Entry at Open of (base_idx + 1 trading days)\n",
    "        if base_idx + 1 < len(ticker_trading_days):\n",
    "            df_events_cleaned.loc[index, 'Actual_Trade_Entry_Date_Initial'] = ticker_trading_days[base_idx + 1]\n",
    "\n",
    "        # 3_Day_Return (Open Day1 to Close Day3) is known at Close of (base_idx + 2 trading days)\n",
    "        # Day1 is base_idx, Day2 is base_idx+1, Day3 is base_idx+2\n",
    "        if base_idx + 2 < len(ticker_trading_days):\n",
    "            df_events_cleaned.loc[index, 'Resizing_Decision_Date'] = ticker_trading_days[base_idx + 2]\n",
    "            # Actual resizing trade at Open of (base_idx + 3 trading days)\n",
    "            if base_idx + 3 < len(ticker_trading_days):\n",
    "                 df_events_cleaned.loc[index, 'Actual_Resizing_Trade_Date'] = ticker_trading_days[base_idx + 3]\n",
    "\n",
    "    except KeyError: continue\n",
    "    except Exception as e: print(f\"Error prepping event {index} for {ticker}: {e}\"); continue\n",
    "\n",
    "df_events_cleaned.dropna(subset=['Initial_Day_Return', 'pred_max_drawdown1',\n",
    "                                 'Actual_Trade_Entry_Date_Initial', 'Trade_Date'], inplace=True)\n",
    "# Ensure necessary dates for resizing are valid if total_event_days > 6\n",
    "mask_long_events = df_events_cleaned['Total_Event_Trading_Days'] > 6\n",
    "df_events_cleaned_long_needs_resize_dates = df_events_cleaned[mask_long_events].dropna(\n",
    "    subset=['3_Day_Return', 'pred_max_drawdown3', 'Resizing_Decision_Date', 'Actual_Resizing_Trade_Date']\n",
    ")\n",
    "df_events_cleaned_short_no_resize_dates = df_events_cleaned[~mask_long_events]\n",
    "df_events_cleaned = pd.concat([df_events_cleaned_short_no_resize_dates, df_events_cleaned_long_needs_resize_dates])\n",
    "\n",
    "df_events_cleaned = df_events_cleaned[df_events_cleaned['Trade_Date'] >= df_events_cleaned['Actual_Trade_Entry_Date_Initial']].copy()\n",
    "df_events_cleaned.sort_values(by='Actual_Trade_Entry_Date_Initial', inplace=True)\n",
    "\n",
    "print(len(df_events_cleaned))\n",
    "\n",
    "# Prepare FRED rates (Ensure 'Date' is index)\n",
    "fred_rates_daily = fred_rates.copy()\n",
    "if not isinstance(fred_rates_daily.index, pd.DatetimeIndex):\n",
    "    fred_rates_daily.index = pd.to_datetime(fred_rates_daily.index)\n",
    "fred_rates_daily = fred_rates_daily[~fred_rates_daily.index.duplicated(keep='first')].sort_index()\n",
    "min_date_needed = stock_price_data_multi.index.get_level_values('Date').min()\n",
    "max_date_needed = stock_price_data_multi.index.get_level_values('Date').max()\n",
    "fred_rates_daily = fred_rates_daily.reindex(pd.date_range(min_date_needed, max_date_needed, freq='D')).ffill()\n",
    "\n",
    "fred_rates_daily['Daily_Long_Cost_Rate'] = (fred_rates_daily['Fed_Funds_Rate'] + LONG_FINANCING_SPREAD_ANNUAL) / 365.0\n",
    "fred_rates_daily['Daily_Short_Cost_Rate'] = (fred_rates_daily['Fed_Funds_Rate'] + SHORT_FINANCING_SPREAD_ANNUAL) / 365.0 # Cost to borrow for shorts\n",
    "\n",
    "\n",
    "# --- 2. Backtesting Loop Initialization ---\n",
    "portfolio_log = []\n",
    "trades_log = []\n",
    "open_positions = {} # {ticker: {shares: X, entry_price: Y, direction: 'long'/'short', ...}}\n",
    "\n",
    "sim_start_date = df_events_cleaned['Actual_Trade_Entry_Date_Initial'].min() if not df_events_cleaned.empty else None\n",
    "sim_end_date = df_events_cleaned['Trade_Date'].max() if not df_events_cleaned.empty else None\n",
    "\n",
    "if sim_start_date is None or pd.isna(sim_start_date):\n",
    "    print(\"No valid events to backtest after date preparation.\")\n",
    "    # exit()\n",
    "else:\n",
    "    print(f\"Starting backtest from {sim_start_date.date()} to {sim_end_date.date()}...\")\n",
    "\n",
    "all_simulation_trading_days = stock_price_data_multi.index.get_level_values('Date').unique().sort_values()\n",
    "simulation_days_slice = all_simulation_trading_days[\n",
    "    (all_simulation_trading_days >= sim_start_date) & (all_simulation_trading_days <= sim_end_date)\n",
    "]\n",
    "\n",
    "current_gross_value_longs = 0.0\n",
    "current_gross_value_shorts = 0.0\n",
    "cumulative_closed_trades_net_pnl = 0.0\n",
    "\n",
    "# --- 3. Backtesting Loop ---\n",
    "for today_dt in simulation_days_slice:\n",
    "    daily_realized_pnl_from_exits = 0.0\n",
    "    daily_financing_cost_total = 0.0\n",
    "    daily_tcost_total = 0.0\n",
    "\n",
    "    # --- A. Apply Financing Costs ---\n",
    "    for ticker, pos_details in open_positions.items():\n",
    "        cost_rate_col = 'Daily_Long_Cost_Rate' if pos_details['direction'] == 'long' else 'Daily_Short_Cost_Rate'\n",
    "        try:\n",
    "            daily_cost_rate = fred_rates_daily.loc[today_dt, cost_rate_col]\n",
    "        except KeyError:\n",
    "            daily_cost_rate = fred_rates_daily[cost_rate_col].asof(today_dt)\n",
    "\n",
    "        if pd.isna(daily_cost_rate): # Fallback\n",
    "            daily_cost_rate = (0.0 + (LONG_FINANCING_SPREAD_ANNUAL if pos_details['direction'] == 'long' else SHORT_FINANCING_SPREAD_ANNUAL)) / 365.0\n",
    "\n",
    "        financing_cost_today = abs(pos_details['value_at_prev_eod']) * daily_cost_rate # abs for shorts\n",
    "        pos_details['current_financing_cost'] += financing_cost_today\n",
    "        daily_financing_cost_total += financing_cost_today\n",
    "\n",
    "    # --- B. Process Exits --- (Similar to before, but handle short P&L)\n",
    "    positions_to_remove_today = []\n",
    "    for ticker, pos_details in open_positions.items():\n",
    "        if today_dt >= pos_details['target_exit_date']: # Use >= for safety if exact date missed\n",
    "            try:\n",
    "                exit_price = stock_price_data_multi.loc[(today_dt, ticker), 'Close']\n",
    "                if pd.isna(exit_price): print(f\"Warning: Exit price NaN for {ticker} on {today_dt}. Holding.\"); continue\n",
    "\n",
    "                tcost_exit = TRANSACTION_COST_PER_SHARE * abs(pos_details['shares'])\n",
    "\n",
    "                if pos_details['direction'] == 'long':\n",
    "                    proceeds_gross = pos_details['shares'] * exit_price\n",
    "                    pnl_gross_trade = proceeds_gross - pos_details['entry_value_gross']\n",
    "                    current_gross_value_longs -= pos_details['entry_value_gross']\n",
    "                else: # Short\n",
    "                    cost_to_cover_gross = abs(pos_details['shares']) * exit_price\n",
    "                    # PnL for short = Initial Proceeds (entry_value_gross) - Cost to Cover\n",
    "                    pnl_gross_trade = pos_details['entry_value_gross'] - cost_to_cover_gross\n",
    "                    current_gross_value_shorts -= pos_details['entry_value_gross']\n",
    "\n",
    "\n",
    "                pnl_net_trade = pnl_gross_trade - pos_details['tcost_entry'] - tcost_exit - pos_details['current_financing_cost']\n",
    "\n",
    "                trades_log.append({\n",
    "                    'ticker': ticker, 'direction': pos_details['direction'],\n",
    "                    'entry_date': pos_details['entry_date'], 'exit_date': today_dt,\n",
    "                    'shares': pos_details['shares'], 'entry_price': pos_details['entry_price'], 'exit_price': exit_price,\n",
    "                    'pnl_net_all_costs': pnl_net_trade, 'tcost_total': pos_details['tcost_entry'] + tcost_exit,\n",
    "                    'total_financing_cost': pos_details['current_financing_cost']\n",
    "                })\n",
    "                daily_realized_pnl_from_exits += pnl_net_trade\n",
    "                positions_to_remove_today.append(ticker)\n",
    "            except KeyError: print(f\"Data missing for {ticker} on scheduled exit {today_dt}. Holding.\"); continue\n",
    "    for ticker_to_remove in positions_to_remove_today: del open_positions[ticker_to_remove]\n",
    "\n",
    "\n",
    "    # --- C. Process Re-Sizing Trades ---\n",
    "    # This happens on Actual_Resizing_Trade_Date at Open\n",
    "    events_for_today_resizing = df_events_cleaned[\n",
    "        (df_events_cleaned['Actual_Resizing_Trade_Date'] == today_dt) &\n",
    "        (df_events_cleaned['Total_Event_Trading_Days'] > 6)\n",
    "    ]\n",
    "    for _, event in events_for_today_resizing.iterrows():\n",
    "        ticker = event['Normalized_Ticker']\n",
    "        if ticker in open_positions: # Position should exist if we are resizing\n",
    "            pos_details = open_positions[ticker]\n",
    "            pred_max_dd3 = event['pred_max_drawdown3'] # This must be > 0 for sizing logic\n",
    "            if pd.isna(pred_max_dd3) or pred_max_dd3 <= 0: continue\n",
    "\n",
    "            try:\n",
    "                current_price_open = stock_price_data_multi.loc[(today_dt, ticker), 'Open']\n",
    "                adv_20 = stock_price_data_multi.loc[(today_dt, ticker), 'ADV_20']\n",
    "                if pd.isna(current_price_open) or pd.isna(adv_20) or current_price_open <= 0 or adv_20 <=0: continue\n",
    "\n",
    "                target_value_new = RISK_LIMIT_PER_TRADE_VALUE / pred_max_dd3 # abs is handled by pred_max_dd being positive\n",
    "                target_shares_new_abs = np.floor(target_value_new / current_price_open)\n",
    "\n",
    "                # Apply liquidity cap\n",
    "                max_shares_by_liquidity = LIQUIDITY_ADV_PERCENT_CAP * adv_20\n",
    "                target_shares_new_abs = min(target_shares_new_abs, max_shares_by_liquidity)\n",
    "\n",
    "                current_shares_abs = abs(pos_details['shares'])\n",
    "                shares_to_trade_abs = abs(target_shares_new_abs - current_shares_abs)\n",
    "\n",
    "                if shares_to_trade_abs > 0:\n",
    "                    # Check portfolio gross value limit before resizing\n",
    "                    # This is simplified: assumes resizing doesn't drastically flip overall portfolio exposure direction\n",
    "                    additional_gross_value_change = shares_to_trade_abs * current_price_open\n",
    "\n",
    "                    # Check if adding this change exceeds overall portfolio limit. Gross value calculation is tricky with shorts.\n",
    "                    # For simplicity, let's check against a portion of available capital if it's an increase.\n",
    "                    # A more robust check would look at total gross (abs longs + abs shorts)\n",
    "\n",
    "                    # Assume direction doesn't change due to resizing for now\n",
    "                    new_total_shares_signed = target_shares_new_abs * np.sign(pos_details['shares'])\n",
    "\n",
    "                    # Update portfolio gross value trackers\n",
    "                    if pos_details['direction'] == 'long':\n",
    "                        current_gross_value_longs -= pos_details['entry_value_gross'] # Remove old value\n",
    "                        current_gross_value_longs += new_total_shares_signed * current_price_open # Add new value\n",
    "                    else: # short\n",
    "                        current_gross_value_shorts -= pos_details['entry_value_gross']\n",
    "                        current_gross_value_shorts += abs(new_total_shares_signed * current_price_open)\n",
    "\n",
    "\n",
    "                    if (current_gross_value_longs + current_gross_value_shorts) > INITIAL_PORTFOLIO_VALUE * 1.1: # Allow some leeway\n",
    "                        # print(f\"Portfolio capacity limit on resizing for {ticker}, skipping resize.\")\n",
    "                        # Revert value changes if resize skipped\n",
    "                        if pos_details['direction'] == 'long':\n",
    "                            current_gross_value_longs -= new_total_shares_signed * current_price_open\n",
    "                            current_gross_value_longs += pos_details['entry_value_gross']\n",
    "                        else:\n",
    "                            current_gross_value_shorts -= abs(new_total_shares_signed * current_price_open)\n",
    "                            current_gross_value_shorts += pos_details['entry_value_gross']\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    tcost_resize = TRANSACTION_COST_PER_SHARE * shares_to_trade_abs\n",
    "                    daily_tcost_total += tcost_resize\n",
    "\n",
    "                    # Update position: adjust average entry price, shares, entry_value_gross, tcost_entry\n",
    "                    # This is complex. For simplicity, let's assume the 'entry_value_gross' reflects the new state for financing.\n",
    "                    # A full P&L attribution for resizing would be more involved.\n",
    "                    # Here, we just update shares and the cost basis for financing.\n",
    "                    pos_details['shares'] = new_total_shares_signed\n",
    "                    pos_details['tcost_entry'] += tcost_resize # Add resizing tcost\n",
    "                    # Recalculate entry_value_gross based on new shares and current price for future reference / financing\n",
    "                    pos_details['entry_value_gross'] = abs(new_total_shares_signed * current_price_open)\n",
    "                    # Note: Average entry price would change. Not explicitly tracking here for simplicity.\n",
    "                    print(f\"Resized {ticker} on {today_dt} to {pos_details['shares']} shares.\")\n",
    "            except KeyError: continue\n",
    "\n",
    "    # --- D. Process New Initial Entries ---\n",
    "    events_for_today_initial_entry = df_events_cleaned[df_events_cleaned['Actual_Trade_Entry_Date_Initial'] == today_dt]\n",
    "    for _, event in events_for_today_initial_entry.iterrows():\n",
    "        ticker = event['Normalized_Ticker']\n",
    "        if ticker in open_positions:\n",
    "            print(\"problem\")\n",
    "            continue # Already have a position (e.g. from another event, unlikely here)\n",
    "\n",
    "        initial_day_return_signal = event['Initial_Day_Return']\n",
    "        pred_max_dd1 = event['pred_max_drawdown1'] # This must be > 0 for sizing logic\n",
    "\n",
    "        if pd.isna(initial_day_return_signal) or pd.isna(pred_max_dd1):\n",
    "            print(\"big not good\")\n",
    "            continue\n",
    "\n",
    "        trade_direction = None\n",
    "        if initial_day_return_signal > 0: trade_direction = 'long'\n",
    "        elif initial_day_return_signal < 0: trade_direction = 'short'\n",
    "        else: continue # No trade if signal is zero\n",
    "\n",
    "        try:\n",
    "            entry_price = stock_price_data_multi.loc[(today_dt, ticker), 'Open']\n",
    "            adv_20 = stock_price_data_multi.loc[(today_dt, ticker), 'ADV_20']\n",
    "            if pd.isna(entry_price) or pd.isna(adv_20) or entry_price <=0 or adv_20 <=0: continue\n",
    "\n",
    "            if trade_direction == 'long':\n",
    "                if pred_max_dd1 <= 0:\n",
    "                    max_dd = pred_max_dd1 * LIQUIDITY_ADV_PERCENT_CAP * adv_20 * entry_price\n",
    "                    if max_dd <= -RISK_LIMIT_PER_TRADE_VALUE:\n",
    "                        num_shares_abs = np.floor(RISK_LIMIT_PER_TRADE_VALUE / (entry_price * pred_max_dd1))\n",
    "            else:\n",
    "                if pred_max_dd1 >= 0:\n",
    "                    max_dd = -pred_max_dd1 * LIQUIDITY_ADV_PERCENT_CAP * adv_20 * entry_price\n",
    "                    if max_dd <= -RISK_LIMIT_PER_TRADE_VALUE:\n",
    "                        num_shares_abs = np.floor(RISK_LIMIT_PER_TRADE_VALUE / (entry_price * pred_max_dd1))\n",
    "\n",
    "            #target_value = RISK_LIMIT_PER_TRADE_VALUE / pred_max_dd1\n",
    "            #num_shares_abs = np.floor(min(target_value / entry_price, LIQUIDITY_ADV_PERCENT_CAP * adv_20))\n",
    "\n",
    "            if num_shares_abs <= 0: continue\n",
    "\n",
    "            num_shares_signed = num_shares_abs if trade_direction == 'long' else -num_shares_abs\n",
    "            entry_value_gross_this_trade = num_shares_abs * entry_price # Always positive value\n",
    "            tcost_entry_this_trade = TRANSACTION_COST_PER_SHARE * num_shares_abs\n",
    "\n",
    "            # Check portfolio capacity\n",
    "            temp_new_gross_long = current_gross_value_longs + (entry_value_gross_this_trade if trade_direction == 'long' else 0)\n",
    "            temp_new_gross_short = current_gross_value_shorts + (entry_value_gross_this_trade if trade_direction == 'short' else 0)\n",
    "\n",
    "            if (temp_new_gross_long + temp_new_gross_short) <= INITIAL_PORTFOLIO_VALUE :\n",
    "                open_positions[ticker] = {\n",
    "                    'shares': num_shares_signed, 'entry_price': entry_price, 'direction': trade_direction,\n",
    "                    'entry_date': today_dt, 'target_exit_date': event['Trade_Date'],\n",
    "                    'entry_value_gross': entry_value_gross_this_trade, # Used for P&L calc for shorts, and financing base\n",
    "                    'tcost_entry': tcost_entry_this_trade,\n",
    "                    'current_financing_cost': 0.0,\n",
    "                    'value_at_prev_eod': entry_value_gross_this_trade,\n",
    "                    'event_total_days': event['Total_Event_Trading_Days'], # For resizing logic\n",
    "                    'pred_max_drawdown3': event['pred_max_drawdown3'] # For resizing\n",
    "                }\n",
    "                if trade_direction == 'long': current_gross_value_longs += entry_value_gross_this_trade\n",
    "                else: current_gross_value_shorts += entry_value_gross_this_trade\n",
    "                daily_tcost_total += tcost_entry_this_trade\n",
    "            # else: print(f\"Portfolio capacity limit on initial entry for {ticker}, skipping.\")\n",
    "        except KeyError: continue\n",
    "\n",
    "    # --- E. Mark-to-Market open positions ---\n",
    "    current_holdings_mtm_value = 0.0\n",
    "    for ticker, pos_details in open_positions.items():\n",
    "        try:\n",
    "            current_eod_price = stock_price_data_multi.loc[(today_dt, ticker), 'Close']\n",
    "            if pd.isna(current_eod_price): current_eod_value = pos_details['value_at_prev_eod']\n",
    "            else: current_eod_value = abs(pos_details['shares']) * current_eod_price # MTM is always positive value\n",
    "\n",
    "            pos_details['value_at_prev_eod'] = current_eod_value\n",
    "            current_holdings_mtm_value += current_eod_value\n",
    "        except KeyError:\n",
    "            current_holdings_mtm_value += pos_details.get('value_at_prev_eod', pos_details['entry_value_gross'])\n",
    "\n",
    "    # --- F. Log Daily Portfolio Status ---\n",
    "    cumulative_closed_trades_net_pnl += daily_realized_pnl_from_exits\n",
    "    total_current_gross_exposure = current_gross_value_longs + current_gross_value_shorts\n",
    "\n",
    "    portfolio_log.append({\n",
    "        'date': today_dt,\n",
    "        'portfolio_value': INITIAL_PORTFOLIO_VALUE + cumulative_closed_trades_net_pnl,\n",
    "        'gross_exposure_total': total_current_gross_exposure,\n",
    "        'gross_long': current_gross_value_longs, 'gross_short': current_gross_value_shorts,\n",
    "        'open_positions_mtm_value': current_holdings_mtm_value,\n",
    "        'num_open_positions': len(open_positions),\n",
    "        'daily_closed_trades_pnl': daily_realized_pnl_from_exits,\n",
    "        'daily_financing_cost_total': daily_financing_cost_total,\n",
    "        'daily_tcost_total': daily_tcost_total\n",
    "    })\n",
    "\n",
    "    if today_dt.day % 10 == 0 or today_dt.is_month_end : # More frequent logging for debug\n",
    "         if portfolio_log: # Check if log has entries\n",
    "            print(f\"Processed {today_dt.date()}. PortVal: ${portfolio_log[-1]['portfolio_value']:,.0f}. OpenPos: {len(open_positions)}. GrossExp: ${total_current_gross_exposure:,.0f}\")\n",
    "\n",
    "\n",
    "# --- 4. Performance Analysis (adapted for long/short) ---\n",
    "# (The performance analysis part will be largely similar but might need slight adjustments\n",
    "#  to separately analyze long vs short trades from trades_log if desired)\n",
    "# ... (Plotting and summary stats as in previous backtester script) ...\n",
    "# (Ensure the plotting for gross exposure uses the new 'gross_exposure_total')\n",
    "df_portfolio_log = pd.DataFrame(portfolio_log)\n",
    "if not df_portfolio_log.empty: df_portfolio_log.set_index('date', inplace=True)\n",
    "df_trades_log = pd.DataFrame(trades_log)\n",
    "\n",
    "print(\"\\n--- Backtest Summary (New Logic) ---\")\n",
    "if not df_trades_log.empty:\n",
    "    total_net_pnl_all_trades = df_trades_log['pnl_net_all_costs'].sum()\n",
    "    num_total_trades = len(df_trades_log)\n",
    "\n",
    "    # Win rate based on PnL > 0\n",
    "    winning_trades_df = df_trades_log[df_trades_log['pnl_net_all_costs'] > 0]\n",
    "    num_winning_trades = len(winning_trades_df)\n",
    "\n",
    "    # Long/Short specific PnL\n",
    "    pnl_long = df_trades_log[df_trades_log['direction']=='long']['pnl_net_all_costs'].sum()\n",
    "    pnl_short = df_trades_log[df_trades_log['direction']=='short']['pnl_net_all_costs'].sum()\n",
    "    num_long = len(df_trades_log[df_trades_log['direction']=='long'])\n",
    "    num_short = len(df_trades_log[df_trades_log['direction']=='short'])\n",
    "\n",
    "    print(f\"Final Portfolio Value: ${INITIAL_PORTFOLIO_VALUE + total_net_pnl_all_trades:,.2f}\")\n",
    "    print(f\"Total Net PnL from Closed Trades: ${total_net_pnl_all_trades:,.2f}\")\n",
    "    print(f\"  PnL from Longs: ${pnl_long:,.2f} (from {num_long} trades)\")\n",
    "    print(f\"  PnL from Shorts: ${pnl_short:,.2f} (from {num_short} trades)\")\n",
    "    print(f\"Total Trades Executed: {num_total_trades}\")\n",
    "\n",
    "    if num_total_trades > 0:\n",
    "        print(f\"Overall Win Rate: {num_winning_trades / num_total_trades:.2%}\")\n",
    "        # Win rate for longs\n",
    "        if num_long > 0:\n",
    "            win_rate_long = len(winning_trades_df[winning_trades_df['direction']=='long']) / num_long\n",
    "            print(f\"  Win Rate Longs: {win_rate_long:.2%}\")\n",
    "        # Win rate for shorts\n",
    "        if num_short > 0:\n",
    "            win_rate_short = len(winning_trades_df[winning_trades_df['direction']=='short']) / num_short\n",
    "            print(f\"  Win Rate Shorts: {win_rate_short:.2%}\")\n",
    "\n",
    "        print(f\"Average PnL per Trade: ${total_net_pnl_all_trades / num_total_trades:,.2f}\")\n",
    "        # ... (rest of avg win/loss, total costs similar to before) ...\n",
    "        total_tcosts = df_trades_log['tcost_total'].sum()\n",
    "        total_fincosts = df_trades_log['total_financing_cost'].sum()\n",
    "        print(f\"Total Transaction Costs (from closed trades): ${total_tcosts:,.2f}\")\n",
    "        print(f\"Total Financing Costs (from closed trades): ${total_fincosts:,.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No trades were executed in the backtest.\")\n",
    "\n",
    "if not df_portfolio_log.empty:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    df_portfolio_log['portfolio_value'].plot(label='Portfolio Value (Initial + Closed PnL)')\n",
    "    plt.title('Portfolio Value Over Time (New Logic)')\n",
    "    plt.xlabel('Date'); plt.ylabel('Portfolio Value ($)')\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    df_portfolio_log[['gross_long', 'gross_short', 'gross_exposure_total']].plot(label=['Gross Longs', 'Gross Shorts', 'Total Gross Exposure'])\n",
    "    plt.axhline(INITIAL_PORTFOLIO_VALUE, color='r', linestyle='--', label=f'Max Portfolio Size (${INITIAL_PORTFOLIO_VALUE:,.0f})')\n",
    "    plt.title('Portfolio Gross Exposure Over Time (New Logic)')\n",
    "    plt.xlabel('Date'); plt.ylabel('Exposure ($)')\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"Portfolio log is empty, cannot plot performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
